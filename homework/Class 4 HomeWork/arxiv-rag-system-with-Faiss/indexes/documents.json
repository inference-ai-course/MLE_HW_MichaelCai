[
  {
    "text": "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training Marc Brinner and Sina Zarrieß Computational Linguistics, Department of Linguistics Bielefeld University, Germany marc.brinner,sina.zarriessuni-bielefeld.de Abstract We propose an end-to-end differentiable train- ing paradigm for stable training of a rational- ized transformer classifier. Our approach re- sults in a single model that simultaneously clas- sifies a sample and scores input tokens based on their relevance to the classification. To this end, we build on the widely-used three-player- game for training rationalized models, which typically relies on training a rationale selec- tor, a classifier and a complement classifier. We simplify this approach by making a single model fulfill all three roles, leading to a more efficient training paradigm that is not suscep- tible to the common training instabilities that plague existing approaches. Further, we extend this paradigm to produce class-wise rationales while incorporating recent advances in param- eterizing and regularizing the resulting ratio- nales, thus leading to substantially improved and state-of-the-art alignment with human an- notations without any explicit supervision. 1 Introduction Neural networks are increasingly prevalent across a wide range of applications, driving significant advancements in fields such as natural language processing, computer vision, and beyond. Due to the black-box nature of these networks, this widespread use comes with an increased demand for interpretability (Lyu et al., 2024), as understand- ing the basis for the decisions made by these mod- els is crucial for their reliable and ethical deploy- ment. This need has become especially clear with the increasing use of notoriously uninterpretable large language models, which have the potential to quickly lose a users trust after only few confidently incorrect predictions (Dhuliawala et al., 2023). One possible mitigation is the use of encoder- only models, which lend themselves more readily to classical interpretability approaches designed for general neural network classifiers while still provid- ing state-of-the-art performance due to continuous improvements in model structure (He et al., 2021, 2023) and training paradigms (Zhang et al., 2023). While a variety of explainability methods ex- ist, that usually assign scores to input tokens indi- cating their importance for a classification (Sun et al., 2021), these methods often suffer from several drawbacks, including high computational cost, difficult-to-interpret explanations, and poten- tially even unfaithful representations of the models decision-making process. In this study, we close this gap by developing a rationalized transformer predictor that generates faithful and interpretable explanations in addition to its decisions within the same forward pass. As a foundation for our approach, we build upon the existing and commonly used three-player game proposed by Yu et al. (2019). In this framework, a selector model chooses a subset of the input as ratio- nale, while a predictor and a complement predictor model are trained to infer the correct label from either the tokens included in the rationale or the tokens not included in the rationale, respectively. The selector model is then trained to maximally aid the predictor in predicting the correct label while preventing the complement predictor from doing the same, thus ensuring that all tokens indicative of the correct label are included in the rationale. While the",
    "metadata": {
      "document_id": 0,
      "chunk_id": 0,
      "source": "pdf_0",
      "chunk_length": 3552
    },
    "id": 0
  },
  {
    "text": "the tokens not included in the rationale, respectively. The selector model is then trained to maximally aid the predictor in predicting the correct label while preventing the complement predictor from doing the same, thus ensuring that all tokens indicative of the correct label are included in the rationale. While the general three-player game is sensible, the actual realizations that are proposed often have several limitations, including being not end-to-end differentiable due to a stochastic sampling process in the forward pass, showing interlocking dynam- ics that might prevent convergence to a suitable solution, and having no guarantee of providing a rationale that actually explains the prediction (com- pare Section 2.3 for a more detailed discussion). For this reason, we propose a new take on this three-player game that is not susceptible to these drawbacks. We achieve this by making use of a single unified model that is trained as a standard classifier on the complete unaltered input, while 11894 Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1189411907 November 12-16, 2024 2024 Association for Computational Linguistics arXiv2508.11393v1 cs.CL 15 Aug 2025 simultaneously predicting class-wise importance scores for each input token in the same forward pass, which are then trained using self-training to mark spans that the model itself considers impor- tant for the specific class. Our proposed rationalized transformer predic- tor (RTP) simplifies and enhances the common three-player structure in several ways, including 1) using only a single model to fulfill all three roles of the three-player game, thus enabling classification and rationale prediction in a single forward pass 2) training the rationales to explain the predictor, but avoiding training the predictor on the rationales, which ensures that the rationales faithfully explain the predictions 3) creating rationalized inputs in continuous fashion to enable fully differentiable training and avoid sampling 4) creating class-wise rationales and 5) using a parameterization that max- imizes similarity with human rationale annotations. We evaluate our method on two benchmarks for explainable AI and compare it with existing post-hoc explanation methods as well as methods leveraging standard multi-player procedures. We show that our method achieves state-of-the-art per- formance on both tasks, demonstrating previously unseen alignment with human rationales in combi- nation with high rationale faithfulness. 2 Background 2.1 Post-Hoc Rationalization Since neural networks are black-box models, the ever-increasing use of such models in research and industry has led to a strong demand for methods that reliably explain neural network classifications. To this end, a variety of approaches have been pro- posed, many of which are designed to create post- hoc explanations for an already trained classifier. These methods rely on a variety of mechanisms, including 1) making use of the models gradients at different inputs to obtain importance scores (Si- monyan et al., 2013 Sundararajan et al., 2017) 2) quantifying the influence of individual input ele- ments by observing the effect of input perturba- tions on the predicted outputs (Castro et al., 2009 Zeiler and Fergus, 2014 Zhou et al., 2014 Pet- siuk et al., 2018) 3) fitting interpretable models to neural-network outputs (Ribeiro et",
    "metadata": {
      "document_id": 0,
      "chunk_id": 1,
      "source": "pdf_0",
      "chunk_length": 3432
    },
    "id": 1
  },
  {
    "text": "et al., 2017) 2) quantifying the influence of individual input ele- ments by observing the effect of input perturba- tions on the predicted outputs (Castro et al., 2009 Zeiler and Fergus, 2014 Zhou et al., 2014 Pet- siuk et al., 2018) 3) fitting interpretable models to neural-network outputs (Ribeiro et al., 2016) 4) de- veloping backpropagation-like procedures to prop- agate importance information from the model out- put to the input features (Zeiler and Fergus, 2014 Springenberg et al., 2015 Bach et al., 2015 Shriku- mar et al., 2017 Chefer et al., 2021a,b) 5) perform- ing input-optimization to create an altered input that only retains the information important for the classification (Brinner and Zarrieß, 2023). 2.2 Rationalized Classification Due to the inherent difficulty of creating post-hoc explanations for classifiers that were never de- signed to be explainable, rationalized predictors have been proposed that are explicitly trained to perform the original task while simultaneously pro- viding a rationale for the prediction in a single for- ward pass. Lei et al. (2016) were the first to propose a two-player game for textual inputs, involving a rationale selector model and a classifier model. The rationale selector assigns a probability to each in- put word, indicating its likelihood of belonging to the rationale, so that a discrete rationale can be sam- pled from this distribution. The classifier then uses only the rationale to make its classification, thus ensuring that the selected words were responsible for the classification. During training, the classifier is trained as usual to predict the correct label from a sampled rationale, while the rationale selector is trained to produce rationales that aid the classifier in making the correct predictions, ensuring that words indicative of the correct class are selected. 2.3 Common Issues of Rationalized Classifiers While the general training paradigm of the two- player game is sensible, several issues affect the training, performance and faithfulness of the ratio- nales 1. Stochastic Sampling Training requires stochastic sampling of rationales, meaning that gradients can only be estimated using methods like REINFORCE (Williams, 1992), which are generally less stable and slow to convergence. 2. Class-Independent Rationales A single ra- tionale is predicted regardless of the samples class. In case a sample belongs to multiple classes, it is not possible to identify which part of the input is indicative of a specific class. 3. Interlocking Dynamics Interlocking dynam- ics might lead to degenerate solutions, for ex- ample, if the rationale predictor adapts too quickly to the noisy rationales that are pro- duced by the randomly initialized rationale selector or vice versa (Yu et al., 2021). 11895 4. Dominant Selector The training paradigm enforces rationales that persuade the classifier to predict a label that the predictor deemed cor- rect, which does not necessarily correspond to faithful explanations of the actual reason- ing process (Jacovi and Goldberg, 2021). In extreme cases, the rationale generator might simply encode the correct classification in the rationale (e.g., by selecting a specific kind of token), so that the classifier does not perform any significant reasoning itself. 5. Mismatch with Human Annotations Often, rationales are",
    "metadata": {
      "document_id": 0,
      "chunk_id": 2,
      "source": "pdf_0",
      "chunk_length": 3354
    },
    "id": 2
  },
  {
    "text": "actual reason- ing process (Jacovi and Goldberg, 2021). In extreme cases, the rationale generator might simply encode the correct classification in the rationale (e.g., by selecting a specific kind of token), so that the classifier does not perform any significant reasoning itself. 5. Mismatch with Human Annotations Often, rationales are most useful if they resemble rationales provided by human annotators. De- spite regularizers designed to enforce the se- lection of longer, consecutive spans of text, models often struggle to select spans that match human annotations, since overly strong regularization often overpowers the weak gra- dient signal created by REINFORCE, leading to degenerate solutions (e.g., selecting no to- kens or all tokens). 6. Degraded Classification Performance The actual classification performance often de- grades compared to standard classifiers (Ja- covi and Goldberg, 2021). Several approaches have been proposed to mod- ify or extend the two-player game to address these issues. (Liu et al., 2022) address the dominant selector issue by using a shared encoder for both the selector and the classifier, thus ensuring that both components focus on similar features instead of, in the case of a dominant selector, an encoded message. Yu et al. (2019) instead extended the two-player paradigm into a three-player game by introducing a complement predictor that is trained to predict the correct label from all words not in- cluded in the rationale. The rationale selector is then trained to prevent the complement predictor from identifying the correct class, thus ensuring that all words indicative of the correct class are selected as rationale, addressing the interlocking problem and (in part) the problem of having a domi- nant selector. Chang et al. (2019) propose the CAR framework that uses two encoders and one decoder per class to generate class-wise (and potentially counterfactual) rationales, solving issues 2, 3 and (in part) issue 4. They also use the straight-through gradient estimator (Bengio et al., 2013) instead of using REINFORCE, which addresses issue 1. Liu et al. (2023) make use of multiple generators to mitigate issue 3, while the A2R method (Yu et al., 2021) addresses the same issue by introduc- ing a separate predictor that uses a soft selection of inputs instead of binary thresholding. To our knowledge, our proposed method for training a ra- tionalized classifier is the only one to address all of the issues discussed above. 3 Method We propose a new method for end-to-end differen- tiable training of a rationalized transformer predic- tor (RTP). In the following, plain letters (e.g., x) denote scalars, while bold letters (e.g., x) denote vectors or tensors. We assume a text classification problem with label set Y, and a training set consist- ing of texts x0, ..., xn with corresponding ground truth vectors y0, ..., yn. 3.1 Concept The RTP relies on a single model that, in one for- ward pass, produces both a classification output and class-wise importance scores for each token, denoting how indicative each token is of the respec- tive class. The classification component is trained as a standard classifier, while the token-wise ratio- nales are trained by creating altered inputs",
    "metadata": {
      "document_id": 0,
      "chunk_id": 3,
      "source": "pdf_0",
      "chunk_length": 3259
    },
    "id": 3
  },
  {
    "text": "model that, in one for- ward pass, produces both a classification output and class-wise importance scores for each token, denoting how indicative each token is of the respec- tive class. The classification component is trained as a standard classifier, while the token-wise ratio- nales are trained by creating altered inputs that only retain the important information for each individ- ual class. The quality of these altered inputs (and therefore the quality of the rationales) is judged by the model itself by passing them through the model and observing its classification output. Through this end-to-end differentiable procedure, the ratio- nales are optimized to faithfully explain the model predictions. 3.2 Model Structure The basis of our method is a single model M, that, given an input text x, simultaneously predicts class probabilities y, as well as a mask tensor m y, m M(x) (1) with the mask m being the rationale for the classifi- cation output y. Notably, m consists of Y individ- ual vectors m0, ..., mY1 that constitute individ- ual rationales for each class c Y, with each mc being a vector containing a mask value mc i in the range 0 to 1 for each input token xi, indicating its influence on the predicted likelihood of class c. In practice, the basis for classification output y will be the CLS-token embedding of the transformer clas- sifier, while the mask values m will be calculated from the predicted outputs for each token. 11896 Figure 1 An exemplary output of the RTP for a positive review from the movie reviews dataset. 3.3 Mask Parameterization In this section, we will discuss the parameterization that transforms token-wise neural network outputs into a smooth mask. The RTP outputs a mask for each individual class, but since mask calculations for individual classes are independent of each other, we will look at the mask mc for a single class c, which we denote as m for simplicity. A simple mask parameterization would predict a logit li for each token xi and define mi σ(li). Even with regularizers that enforce smooth mask selections, this approach often fails to select long spans of text as rationales, which would be de- sirable for matching human annotations. For this reason, we opted for the mask parameterization proposed by Brinner and Zarrieß (2023) that ex- plicitly enforces the prediction of longer spans of text as rationales by letting neighboring mask val- ues influence each other. In this parameterization, the model outputs two values wi and σi for each word xi. wi is mainly responsible for determining the mask value of word xi, while σi determines the influence of wi on the mask values of neighboring words. Introducing regularizers to enforce large values for σi then leads to smooth masks. The mathematical formulation of the parameterization is as follows wij wi exp d(i, j)2 σi (2) mj sigmoid( X i wij) (3) Here, d(i, j) denotes the distance between two words xi and xj and wij is the influence of wi on the mask value of word j. mj is then calculated by applying",
    "metadata": {
      "document_id": 0,
      "chunk_id": 4,
      "source": "pdf_0",
      "chunk_length": 3030
    },
    "id": 4
  },
  {
    "text": "the parameterization is as follows wij wi exp d(i, j)2 σi (2) mj sigmoid( X i wij) (3) Here, d(i, j) denotes the distance between two words xi and xj and wij is the influence of wi on the mask value of word j. mj is then calculated by applying the sigmoid to the sum of all influence val- ues, resulting in the mask m for the specific class at hand. Predicting masks m0, ..., mY1 for each class will simply be done by predicting individual outputs wc and σc for each class c and performing the calculations independently. 3.4 Model Training Given a sample (x, y), the classification capabilities of model M are trained like a standard neural net- work classifier by performing a prediction and ap- plying a loss function like cross-entropy loss to the predicted output. In contrast to other rationalized models, our training paradigm therefore trains the classifier on the unaltered input, not on a masked version that might remove crucial information. To train the rationale predictions (i.e., the masks m), we use the current mask predictions to create two altered inputs xc and xc for each ground-truth class c, with input xc retaining all information that is indicative of class c according to mask mc, while xc is the complement input that removes all infor- mation specified by mask mc xc mc x (1 mc) b (4) xc (1 mc) x mc b (5) Here, b denotes an uninformative background (e.g., PAD-token embeddings). Notably, the mask m is applied in continuous fashion to x and b, meaning that embeddings for words are linearly blended towards uninformative embeddings according to m. In contrast to sampling of a discrete mask, this ensures full differentiability and was proven to have the desired effect of gradual removal of information by Brinner and Zarrieß (2023). The rationalized inputs are then fed back into the same model M and are scored by its classification component. We then make use of a loss function that rewards predicting the correct label from xc, but not from xc, meaning that all information in- dicative of class c is contained in the rationale (thus enforcing rationale comprehensiveness). The loss formulations used are the following Lc CE(M(xc), y) (6) Lc relu(M(xc)c α) (7) where CE denotes the cross-entropy loss, M(x)c denotes the predicted probability of class c and α is a hyperparameter ensuring that the model is not required to drive the probability of class c for input xc to 0, since driving it to a small value is sufficient. Importantly, the model M is only trained with re- spect to the first forward pass that produces the rationales, and is therefore not updated to improve 11897 classification on the altered inputs xc and xc. This ensures that the classification performance in not influenced, and that the rationales actually explain the classification instead of dictating it. The final optimization problem looks as follows arg min M CE(M(x),y) X cy Lc Lc Ωλ Ωσ (8) where c y indicates summing over all ground- truth labels and Ωλ and Ωσ",
    "metadata": {
      "document_id": 0,
      "chunk_id": 5,
      "source": "pdf_0",
      "chunk_length": 2978
    },
    "id": 5
  },
  {
    "text": "the classification performance in not influenced, and that the rationales actually explain the classification instead of dictating it. The final optimization problem looks as follows arg min M CE(M(x),y) X cy Lc Lc Ωλ Ωσ (8) where c y indicates summing over all ground- truth labels and Ωλ and Ωσ denote regularizers that enforce sparsity and smoothness of the rationales, respectively. Details on these regularizers and fur- ther details on the general objective are available in Appendix A.3. 3.5 Advantages Our proposed scheme solves all issues discussed in Section 2.3, and is (to our knowledge) the only method to do so. The main advantages lie in the fully differentiable formulation that does not re- quire sampling and gradient approximation, the fact that class-wise rationales are created, and es- pecially in the fact that the classifier is trained on the unaltered inputs instead of on the rationalized variants. This last point ensures that the rationales do not dictate the classification result, but instead explain the actual classification made by the classi- fier, and therefore completely bypasses issues like interlocking or the dominant selector, thus enforc- ing high rationale faithfulness. It also means that no degradation in classification performance is to be expected. 4 Experiments We evaluate our method with regards to matching human evidence annotations for text classifications, as well as with regards to the faithfulness of the ex- planations with regards to the classifier. For details regarding the model and the training and prediction procedures, see Appendix A. 4.1 Datasets In our evaluation, we use two text classification datasets with span-level evidence annotations, each posing different challenges. The first is the movie review dataset (Zaidan et al., 2007), containing 2000 reviews with sentiment labels (positive or negative) and span-level evidence annotations. For this dataset, DeYoung et al. (2020) provided more comprehensive rationales for the test split, which we use in our evaluation. Since class labels are mutually exclusive, this dataset allows models to perform optimally even without class-wise ratio- nales. Additionally, this dataset enables the optimal assessment of the agreement between predicted rationales and the human annotations, since the simplicity of the classification task eliminates the lack of understanding of the inputs as a cause for mismatches. As for a more challenging classification task, we use the INAS dataset (Brinner et al., 2022), consist- ing of 954 scientific paper titles and abstracts from the domain of invasion biology together with labels indicating which hypothesis (from a set of 10 com- mon hypotheses in the field) is addressed in each paper. In a subsequent study, Brinner et al. (2024) provided span-level evidence annotations for 750 of the samples. Since some samples belong to mul- tiple classes, optimal performance on this dataset requires class-wise rationalization. Additionally, the more challenging nature of the classification task can highlight degraded classification perfor- mance of rationalized models. 4.2 Evaluation Metrics We evaluate the consistency with human annota- tions on token-level and span-level as done in (Brin- ner et al., 2024), and evaluate the faithfulness of rationales with respect to the classifier as done in (Brinner and Zarrieß, 2023).",
    "metadata": {
      "document_id": 0,
      "chunk_id": 6,
      "source": "pdf_0",
      "chunk_length": 3382
    },
    "id": 6
  },
  {
    "text": "can highlight degraded classification perfor- mance of rationalized models. 4.2 Evaluation Metrics We evaluate the consistency with human annota- tions on token-level and span-level as done in (Brin- ner et al., 2024), and evaluate the faithfulness of rationales with respect to the classifier as done in (Brinner and Zarrieß, 2023). Token-Level Evaluation To evaluate agreement with human rationales at the token level, we use the area under the precision-recall curve (AUC-PR). We also assess the token-level F1 score (Token-F1), which requires binary predictions. This is done by selecting the highest-scoring p percent of tokens as positive predictions, calculating the standard F1 score, and averaging over 19 values of p (5, 10, ..., 95). For a better absolute assessment of prediction quality, we use the discrete token-level F1 score (D-Token-F1), where the top k tokens are selected as the binary rationale and are evaluated with the F1 score, with k being the number of tokens annotated in the corresponding ground truth. Span-Level Evaluation We also evaluate the quality of predicted spans of text, defined as con- secutive words selected as part of the rationale after binary thresholding. The span-level IoU-F1 score (IoU-F1) is calculated by determining spans in both the binary rationale prediction and the ground-truth annotation, calculating the IoU for all span pairs, and selecting the maximum IoU value for each pre- 11898 Method Clf-F1 AUC-PR Token-F1 D-Token-F1 IoU-F1 D-IoU-F1 Suff. Comp. Perf. Random - 0.220 0.255 0.222 0.067 0.003 0.194 0.191 0.289 Supervised 0.730 0.557 0.406 0.509 0.231 0.257 0.005 0.396 1.028 MaRC 0.776 0.366 0.336 0.351 0.219 0.178 0.040 0.459 0.974 Occlusion 0.776 0.307 0.277 0.294 0.145 0.071 0.078 0.352 0.696 Int. Grads 0.776 0.315 0.302 0.318 0.087 0.013 0.030 0.538 0.897 LIME 0.776 0.272 0.280 0.273 0.082 0.007 0.097 0.406 0.671 Shapley 0.776 0.309 0.301 0.320 0.084 0.009 -0.012 0.587 0.984 L2E-MaRC 0.776 0.431 0.359 0.402 0.174 0.131 0.044 0.503 0.992 2-Player 0.753 0.272 0.286 0.270 0.085 0.007 -0.052 0.367 0.790 3-Player 0.703 0.287 0.296 0.286 0.080 0.004 0.017 0.472 0.831 CAR - 0.314 0.281 0.280 0.184 0.133 - - - A2R 0.654 0.268 0.287 0.264 0.084 0.008 0.128 0.338 0.581 A2R-Noise 0.618 0.258 0.275 0.249 0.081 0.005 0.211 0.408 0.553 RTP 0.777 0.445 0.362 0.416 0.226 0.194 0.088 0.697 1.197 Table 1 Results on the INAS dataset, divided into groups of standard-baselines, post-hoc explainability methods and rationalized neural networks. Best scores per metric are bold, second best are underlined. dicted and annotated span. This effectively speci- fies, how well any predicted or ground-truth span overlaps with a span from the other group. IoU- precision and IoU-recall are then defined as the averages of these maximum IoU values for pre- dicted and ground-truth spans, respectively, from which the usual F1 score can be calculated. The holistic IoU-F1 score is then obtained by averag- ing over the same 19 discrete token selections used for the token-level F1 score. The discrete IoU-F1 score (D-IoU-F1) is again calculated by selecting the top-scoring tokens to match the number of to- kens specified in the ground-truth annotation. Faithfulness Evaluation We evaluate rationale faithfulness using",
    "metadata": {
      "document_id": 0,
      "chunk_id": 7,
      "source": "pdf_0",
      "chunk_length": 3275
    },
    "id": 7
  },
  {
    "text": "then obtained by averag- ing over the same 19 discrete token selections used for the token-level F1 score. The discrete IoU-F1 score (D-IoU-F1) is again calculated by selecting the top-scoring tokens to match the number of to- kens specified in the ground-truth annotation. Faithfulness Evaluation We evaluate rationale faithfulness using scores for sufficiency and com- prehensiveness of the predicted rationales. The sufficiency score measures the models ability to predict the correct label using only the highest- scoring words in the rationale. A lower sufficiency score indicates that fewer tokens are needed for a correct prediction, thus indicating a more faithful rationale sufficiency(x, r) 1 19 19 X i1 M(x) M(ri) (9) The comprehensiveness score is higher if remov- ing the highest-scoring words according to the ra- tionale quickly degrades the models predictions, again indicating faithful rationales comp(x, r) 1 19 19 X i1 M(x) M(xri) (10) In these equations, x denotes the input sample, ri denotes the (i5) of input tokens with the highest scores according to the rationale, xri denotes the input x with the tokens from ri removed, and M(x) denotes the probability that model M assigns to the correct class given input x. To avoid relying on a single threshold, these scores are calculated by summing over different percentages of rationale tokens used or removed, respectively. Overall Performance Ideally, a model should produce rationales that both agree with human ra- tionales and demonstrate faithfulness. We therefore provide an overall performance score (Perf.) that sums over the Token-F1, IoU-F1, comprehensive- ness and negative sufficiency scores, thus assessing agreement and faithfulness comprehensively. 4.3 Baseline Methods We compare our rationalized transformer predictor (RTP) against other rationalized classifiers, which are a two-player game as proposed by Lei et al. (2016), a three-player structure with complement predictor (Yu et al., 2019), the CAR framework for class-wise rationale generation (Chang et al., 2019), and the A2R method (Yu et al., 2021) as well as an extension to it using noise injection (Storek et al., 2023). We also compare post-hoc explain- ability methods that are applied to a standard clas- sifier, which includes MaRC (Brinner and Zarrieß, 2023), Occlusion (Zeiler and Fergus, 2014), Inte- grated Gradients (Sundararajan et al., 2017), LIME (Ribeiro et al., 2016), Shapley value sampling (Cas- tro et al., 2009), as well as a neural network pre- dictor trained on MaRC rationales (L2E-MaRC, Situ et al. (2021)). Finally, we report results for a supervised model trained on rationale annotations and a random predictor as additional baselines. For 11899 Method Clf-F1 AUC-PR Token-F1 D-Token-F1 IoU-F1 D-IoU-F1 Suff. Comp. Perf. Random - 0.316 0.326 0.312 0.061 0.002 0.227 0.238 0.398 Supervised 0.980 0.670 0.514 0.626 0.144 0.169 0.001 0.638 1.295 MaRC 0.965 0.428 0.404 0.423 0.181 0.118 0.036 0.478 1.027 Occlusion 0.965 0.409 0.367 0.377 0.151 0.079 -0.021 0.569 1.108 Int. Grads 0.965 0.376 0.358 0.371 0.067 0.009 0.049 0.484 0.860 LIME 0.965 0.379 0.361 0.369 0.076 0.014 0.005 0.603 1.035 Shapley 0.965 0.442 0.390 0.426 0.082 0.020 -0.029 0.827 1.328 L2E-MaRC 0.965 0.565 0.460 0.534 0.126 0.104 -0.016 0.652 1.254 2-Player 0.930 0.516 0.449",
    "metadata": {
      "document_id": 0,
      "chunk_id": 8,
      "source": "pdf_0",
      "chunk_length": 3320
    },
    "id": 8
  },
  {
    "text": "0.151 0.079 -0.021 0.569 1.108 Int. Grads 0.965 0.376 0.358 0.371 0.067 0.009 0.049 0.484 0.860 LIME 0.965 0.379 0.361 0.369 0.076 0.014 0.005 0.603 1.035 Shapley 0.965 0.442 0.390 0.426 0.082 0.020 -0.029 0.827 1.328 L2E-MaRC 0.965 0.565 0.460 0.534 0.126 0.104 -0.016 0.652 1.254 2-Player 0.930 0.516 0.449 0.508 0.113 0.066 -0.024 0.210 0.796 3-Player 0.955 0.458 0.422 0.465 0.089 0.023 0.003 0.354 0.862 CAR - 0.384 0.364 0.376 0.078 0.013 - - - A2R 0.955 0.474 0.433 0.486 0.111 0.046 0.109 0.320 0.755 A2R-Noise 0.950 0.483 0.440 0.492 0.107 0.044 0.005 0.338 0.880 RTP 0.975 0.567 0.466 0.544 0.203 0.195 -0.029 0.851 1.549 Table 2 Results on the movie reviews dataset, divided into groups of standard-baselines, post-hoc explainability methods and rationalized neural networks. Best scores per metric are bold, second best are underlined. a more detailed overview, see Appendix A.1. 5 Results The results for the evaluation on the INAS dataset and the movie review dataset are displayed in Table 1 and Table 2, respectively. Exemplary predictions are displayed in Figure 1, with further examples being included in Appendix B. 5.1 Classification Performance On both the INAS and movie review datasets, the RTP demonstrates state-of-the-art classification performance, surpassing even the standard classi- fier. While this pattern is consistent across both datasets, we refrain from assuming a general im- provement in classification performance. Instead, we attribute the observed gains to variations in train- ing runs, which are particularly common for the INAS dataset, as reported by Brinner et al. (2022). Notably, all other rationalized classifiers consis- tently exhibit reduced classification performance, thus establishing the RTP as the best-performing model of its kind. 5.2 Token-Level Performance For token-level rationale evaluations on both the INAS and movie review datasets, our RTP method achieves superior performance across AUC-PR, token-F1, and discrete token-F1 metrics. Among competing methods, only L2E-Marc - a neural net- work trained to predict rationales generated by the MaRC method - consistently approaches the RTPs performance. A detailed discussion of the strong performance of these exact two methods is pre- sented in Section 6. Notably, the RTP and other post-hoc methods particularly benefit from the ability to predict class- wise rationales on the INAS dataset. Among ra- tionalized neural networks, only the CAR method shares this capability. This distinction is further underscored by the increased performance of class- agnostic rationalized networks on the movie review dataset, where class-wise rationale prediction is ir- relevant. On this task, the performance gap with the RTP narrows, and some class-agnostic networks even surpass most post-hoc methods. The supervised baseline outperforms all meth- ods in token-level predictions, which is to be ex- pected, given that the weakly supervised methods did not receive any supervision regarding the de- sired rationale output. However, the RTP achieves results that closely approach the supervised base- line across several metrics, demonstrating that in the absence of labeled rationales, the weakly super- vised framework offers an effective alternative. 5.3 Span-Level Performance On both datasets, our RTP method is consistently the best performing method with regards to the IoU- F1 and the discrete",
    "metadata": {
      "document_id": 0,
      "chunk_id": 9,
      "source": "pdf_0",
      "chunk_length": 3407
    },
    "id": 9
  },
  {
    "text": "closely approach the supervised base- line across several metrics, demonstrating that in the absence of labeled rationales, the weakly super- vised framework offers an effective alternative. 5.3 Span-Level Performance On both datasets, our RTP method is consistently the best performing method with regards to the IoU- F1 and the discrete IoU-F1 scores. Compared to the other rationalized methods this is to be expected, since the RTP has been explicitly designed to ex- tract longer spans of text as rationales. In contrast, other rationalized predictors often rely on a total variation regularizer in their optimization objective, which we found to be ineffective since increasing its strength quickly leads to degenerate solutions with either all or none of the words being selected. This highlights the importance of using the MaRC mask parameterization that reliably leads to the de- 11900 sired results. Notably, the RTP comes close to the supervised method on the INAS dataset without any supervision regarding the usual form of human annotations. On the movie review dataset, the RTP even outperforms the supervised method due to the rationales from the test set being more extensive, thus causing a mismatch between training and test data distributions. This shows that even if slightly inaccurate training data is available for a given task, using a weakly supervised method instead might be preferable. 5.4 Faithfulness Results Our RTP method demonstrates competitive perfor- mance with regards to rationale sufficiency, achiev- ing state-of-the-art performance on the movie re- view dataset, while delivering solid but compar- atively weaker results on the INAS dataset. We found that assigning high scores to few important words distributed throughout the whole input is a great strategy for achieving good sufficiency scores (as done, for example, by the Shapley value sam- pling method), since the model can quickly rec- ognize the correct label from these few highly in- dicative words. Our RTP model still performs well despite being explicitly discouraged from pursuing this strategy, indicating that our optimization objec- tive is reasonable for generating faithful rationales. For comprehensiveness, the RTP attains state- of-the-art results on both the INAS and movie re- view datasets, with only the Shapley value sam- pling method being close across both tasks and most other methods being outperformed by a large margin. In general, good faithfulness scores for Shapley value sampling are to be expected, since its objective for scoring input tokens aligns closely with the evaluation measures for faithfulness. Hav- ing our method match or surpass the scores of this method shows the exceptional ability of our learn- ing framework for teaching the model to use the rationales to correctly report on its own reasoning. Overall, our RTP method compares favorably to other rationalized neural networks, since it op- timizes its rationales to actually explain the classi- fication, while other methods might, for example, suffer from issues like a dominant predictor that already dictates a specific label. One additional downside of other rationalized models it that they train the predictor on the rationales, which leads to a constant mismatch between the current predic- tor and the predictor that the",
    "metadata": {
      "document_id": 0,
      "chunk_id": 10,
      "source": "pdf_0",
      "chunk_length": 3328
    },
    "id": 10
  },
  {
    "text": "other methods might, for example, suffer from issues like a dominant predictor that already dictates a specific label. One additional downside of other rationalized models it that they train the predictor on the rationales, which leads to a constant mismatch between the current predic- tor and the predictor that the rationales have been trained to explain. Another important insight is, that post-hoc expla- nation methods do not offer an advantage over the rationales generated by the RTP. Considering, that post-hoc explainers outperform other rationalized networks with respect to faithfulness of the explana- tions, our method is the first all-in-one method that offers both predictions and rationales with state-of- the-art faithfulness in a single forward pass. 5.5 Overall Performance As discussed, the RTP achieves state-of-the-art re- sults in agreement with human rationales and ra- tionale faithfulness, resulting in dominant scores for overall performance (Perf.) on both tasks. In comparison, other rationalized neural networks fall significantly short, with only few post-hoc methods coming somewhat close. These methods have the downside of a substantially higher computational cost in producing a rationale, with, for example, MaRC and Shapley value sampling requiring hun- dreds of forward passes to create a single rationale. 6 Discussion The RTP model demonstrated strong performance across all evaluated metrics. Comparing it specif- ically to the MaRC method, it outperformed it in every metric related to measuring agreement with human annotations and most faithfulness met- rics. This is notable since the RTP can be seen as a neural network parameterized version of the MaRC approach, which originally optimized mask parameters for each sample individually instead of training a neural network to directly predict them from the input. Another well-performing method, especially with regards to token-level eval- uation, is the L2E-MaRC method. The L2E frame- work (Situ et al., 2021) trains a neural network on pre-calculated rationales created by a post-hoc explainer. Even though it only saw rationales pro- duced by the MaRC method, it manages to out- perform it on all metrics measuring token-level agreement with human rationales. These two re- sults indicate, that training to explain many differ- ent samples leads to better generalization, which we attribute to reduced overfitting to one specific input. This effect is crucial for the RTP, since it per- forms input optimization with respect to specific neural network outputs, which has been shown to generally lead to unexpected and uninterpretable ar- tifacts (Simonyan et al., 2013). The MaRC method successfully mitigated this issue by combining con- 11901 strained optimization with heavy regularization, but artifacts (i.e., unexpected spans included in the ra- tionale) are still to be expected. In the case of the RTP, training on many samples further reduces this issue, since these unwanted gradient signals will generally not match between different samples, so that the neural network mainly adapts to the wanted signal that is consistent within larger parts of the training set, and that correspond to features that are generally indicative of the respective class. 7 Conclusion We presented a new method for training a ratio- nalized transformer predictor and demonstrated its strong performance on",
    "metadata": {
      "document_id": 0,
      "chunk_id": 11,
      "source": "pdf_0",
      "chunk_length": 3398
    },
    "id": 11
  },
  {
    "text": "network mainly adapts to the wanted signal that is consistent within larger parts of the training set, and that correspond to features that are generally indicative of the respective class. 7 Conclusion We presented a new method for training a ratio- nalized transformer predictor and demonstrated its strong performance on two natural language pro- cessing benchmarks. Since our proposed training scheme is not invasive to the general training pro- cess and does not produce significant overhead dur- ing prediction, we believe that this approach has the potential to facilitate wider adoption and avail- ability of rationalized predictors. Given that trans- formers are widely used in other modalities like images (Dosovitskiy et al., 2021) and audio data (Verma and Berger, 2021), we hypothesize that our approach can be extended to these modalities and potentially lead to results of similar quality. 8 Limitations While our method for rationalization generally does not interfere with the training of the prediction mod- ule and does not produce notable overhead during prediction, it nevertheless increases the computa- tional cost of model training due to a second for- ward pass through the model, as well as through more training epochs being required due to slower convergence of rationale training compared to the classification component. Additionally, the exact form of the produced ra- tionales depends on the models inner working, so that generally a high overlap with human rationales is not guaranteed in cases where the models rea- soning and human reasoning differ. Finally, while having access to word-level ra- tionale scores is generally helpful, this does not equate to a complete description of the models inner workings and the actual reasoning process, which most likely is impossible to represent in such a simple form. Acknowledgements This work was funded by Deutsche Forschungsge- meinschaft DFG (project number 455913229 T.H., M.B., J.M.J., B.K-R, S.Z.). References Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wo- jciech Samek. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise rele- vance propagation. PLOS ONE, 10(7)146. Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computa- tion. ArXiv, abs1308.3432. Marc Brinner, Tina Heger, and Sina Zarriess. 2022. Linking a hypothesis network from the domain of invasion biology to a corpus of scientific abstracts The INAS dataset. In Proceedings of the first Work- shop on Information Extraction from Scientific Publi- cations, pages 3242, Online. Association for Com- putational Linguistics. Marc Brinner and Sina Zarrieß. 2023. Model inter- pretability and rationale extraction by input mask op- timization. In Findings of the Association for Compu- tational Linguistics ACL 2023, pages 1372213744, Toronto, Canada. Association for Computational Lin- guistics. Marc Brinner, Sina Zarrieß, and Tina Heger. 2024. Weakly supervised claim localization in scientific ab- stracts. In Robust Argumentation Machines (RATIO 2024), Bielefeld, Germany. Springer. Javier Castro, Daniel Gómez, and Juan Tejada. 2009. Polynomial calculation of the shapley value based on sampling. Computers Operations Research, 36(5)17261730. Selected papers presented at the Tenth International Symposium on Locational Deci- sions (ISOLDE X). Shiyu Chang, Yang Zhang, Mo Yu, and Tommi",
    "metadata": {
      "document_id": 0,
      "chunk_id": 12,
      "source": "pdf_0",
      "chunk_length": 3516
    },
    "id": 12
  },
  {
    "text": "Argumentation Machines (RATIO 2024), Bielefeld, Germany. Springer. Javier Castro, Daniel Gómez, and Juan Tejada. 2009. Polynomial calculation of the shapley value based on sampling. Computers Operations Research, 36(5)17261730. Selected papers presented at the Tenth International Symposium on Locational Deci- sions (ISOLDE X). Shiyu Chang, Yang Zhang, Mo Yu, and Tommi Jaakkola. 2019. A game theoretic approach to class-wise se- lective rationalization. In Advances in Neural In- formation Processing Systems, volume 32. Curran Associates, Inc. Hila Chefer, Shir Gur, and Lior Wolf. 2021a. Generic attention-model explainability for interpreting bi- modal and encoder-decoder transformers. In 2021 IEEECVF International Conference on Computer Vision (ICCV), pages 387396. Hila Chefer, Shir Gur, and Lior Wolf. 2021b. Trans- former interpretability beyond attention visualization. In 2021 IEEECVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 782791. 11902 Michael G. Cripps, Graeme W. Bourdôt, David J. Sav- ille, Hariet L. Hinz, Simon V. Fowler, and Grant R. Edwards. 2011. Influence of insects and fungal pathogens on individual and population parameters of cirsium arvense in its native and introduced ranges. Biological Invasions, 13(12)27392754. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. 2020. ERASER A benchmark to evaluate rationalized NLP models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 44434458, Online. Association for Computational Linguistics. Shehzaad Dhuliawala, Vilém Zouhar, Mennatallah El- Assady, and Mrinmaya Sachan. 2023. A diachronic perspective on user trust in AI under uncertainty. In Proceedings of the 2023 Conference on Empir- ical Methods in Natural Language Processing, pages 55675580, Singapore. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words Transformers for image recognition at scale. Preprint, arXiv2010.11929. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2021. Domain-specific lan- guage model pretraining for biomedical natural lan- guage processing. ACM Trans. Comput. Healthcare, 3(1). Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. Debertav3 Improving deberta using electra-style pre- training with gradient-disentangled embedding shar- ing. Preprint, arXiv2111.09543. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta Decoding- enhanced bert with disentangled attention. Preprint, arXiv2006.03654. Alon Jacovi and Yoav Goldberg. 2021. Aligning faithful interpretations with their social attribution. Transac- tions of the Association for Computational Linguis- tics, 9294310. Catherine S. Jarnevich, Thomas J. Stohlgren, David Bar- nett, and John Kartesz. 2006. Filling in the gaps modelling native species richness and invasions us- ing spatially incomplete data. Diversity and Distri- butions, 12(5)511520. Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz-Richardson. 2020. Captum",
    "metadata": {
      "document_id": 0,
      "chunk_id": 13,
      "source": "pdf_0",
      "chunk_length": 3833
    },
    "id": 13
  },
  {
    "text": "nett, and John Kartesz. 2006. Filling in the gaps modelling native species richness and invasions us- ing spatially incomplete data. Diversity and Distri- butions, 12(5)511520. Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz-Richardson. 2020. Captum A unified and generic model interpretability library for pytorch. Preprint, arXiv2009.07896. Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing neural predictions. In Proceedings of the 2016 Conference on Empirical Methods in Nat- ural Language Processing, pages 107117, Austin, Texas. Association for Computational Linguistics. Wei Liu, Haozhao Wang, Jun Wang, Ruixuan Li, Xinyang Li, YuanKai Zhang, and Yang Qiu. 2023. MGR Multi-generator based rationalization. In Pro- ceedings of the 61st Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1 Long Papers), pages 1277112787, Toronto, Canada. As- sociation for Computational Linguistics. Wei Liu, Haozhao Wang, Jun Wang, Ruixuan Li, Chao Yue, and Yuankai Zhang. 2022. Fr Folded rationalization with a unified encoder. Preprint, arXiv2209.08285. Qing Lyu, Marianna Apidianaki, and Chris Callison- Burch. 2024. Towards faithful model explanation in NLP A survey. Computational Linguistics, 50(2). Mark Neumann, Daniel King, Iz Beltagy, and Waleed Ammar. 2019. ScispaCy Fast and Robust Models for Biomedical Natural Language Processing. In Pro- ceedings of the 18th BioNLP Workshop and Shared Task, pages 319327. Association for Computational Linguistics. Vitali Petsiuk, Abir Das, and Kate Saenko. 2018. Rise Randomized input sampling for explanation of black- box models. In British Machine Vision Conference. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. why should i trust you? Explain- ing the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 16, page 11351144, New York, NY, USA. Association for Computing Machinery. Avanti Shrikumar, Peyton Greenside, and Anshul Kun- daje. 2017. Learning important features through propagating activation differences. In Proceedings of the 34th International Conference on Machine Learn- ing, volume 70 of Proceedings of Machine Learning Research, pages 31453153. PMLR. Karen Simonyan, Andrea Vedaldi, and Andrew Zis- serman. 2013. Deep inside convolutional networks Visualising image classification models and saliency maps. CoRR, abs1312.6034. Xuelin Situ, Ingrid Zukerman, Cecile Paris, Sameen Maruf, and Gholamreza Haffari. 2021. Learning to explain Generating stable explanations fast. In Proceedings of the 59th Annual Meeting of the Asso- ciation for Computational Linguistics and the 11th 11903 International Joint Conference on Natural Language Processing (Volume 1 Long Papers), pages 5340 5355, Online. Association for Computational Lin- guistics. Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin A. Riedmiller. 2015. Striving for simplicity The all convolutional net. In 3rd Inter- national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings. Adam Storek, Melanie Subbiah, and Kathleen McKe- own. 2023. Unsupervised selective rationalization with noise injection. In Proceedings of the 61st An- nual Meeting of the Association for Computational Linguistics (Volume 1 Long Papers), pages 12647 12659, Toronto, Canada. Association for Computa- tional Linguistics. Xiaofei Sun, Diyi Yang, Xiaoya Li, Tianwei Zhang, Yux- ian Meng, Han Qiu, Guoyin Wang, Eduard Hovy, and Jiwei Li.",
    "metadata": {
      "document_id": 0,
      "chunk_id": 14,
      "source": "pdf_0",
      "chunk_length": 3685
    },
    "id": 14
  },
  {
    "text": "noise injection. In Proceedings of the 61st An- nual Meeting of the Association for Computational Linguistics (Volume 1 Long Papers), pages 12647 12659, Toronto, Canada. Association for Computa- tional Linguistics. Xiaofei Sun, Diyi Yang, Xiaoya Li, Tianwei Zhang, Yux- ian Meng, Han Qiu, Guoyin Wang, Eduard Hovy, and Jiwei Li. 2021. Interpreting deep learning models in natural language processing A review. Preprint, arXiv2110.10470. Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In Proceed- ings of the 34th International Conference on Machine Learning - Volume 70, ICML17, page 33193328. JMLR.org. Prateek Verma and Jonathan Berger. 2021. Audio trans- formerstransformer architectures for large scale au- dio understanding. adieu convolutions. Preprint, arXiv2105.00335. Ronald J. Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforcement learning. Machine Learning, 8(3)229256. Mo Yu, Shiyu Chang, Yang Zhang, and Tommi Jaakkola. 2019. Rethinking cooperative rationalization In- trospective extraction and complement control. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 4094 4103, Hong Kong, China. Association for Computa- tional Linguistics. Mo Yu, Yang Zhang, Shiyu Chang, and Tommi Jaakkola. 2021. Understanding interlocking dynamics of coop- erative rationalization. In Advances in Neural Infor- mation Processing Systems, volume 34, pages 12822 12835. Curran Associates, Inc. Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using annotator rationales to improve machine learning for text categorization. In Human Language Technologies 2007 The Conference of the North American Chapter of the Association for Computa- tional Linguistics Proceedings of the Main Confer- ence, pages 260267, Rochester, New York. Associa- tion for Computational Linguistics. Matthew D. Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. In Com- puter Vision ECCV 2014, pages 818833, Cham. Springer International Publishing. Zhen-Ru Zhang, Chuanqi Tan, Songfang Huang, and Fei Huang. 2023. Veco 2.0 Cross-lingual language model pre-training with multi-granularity contrastive learning. Preprint, arXiv2304.08205. Bolei Zhou, Aditya Khosla, Àgata Lapedriza, Aude Oliva, and Antonio Torralba. 2014. Object detectors emerge in deep scene cnns. CoRR, abs1412.6856. A Experimental Details The code for our experiments is available at httpsgithub.cominas-argumentation RationalizedTransformerPredictor. A.1 Baseline Methods We evaluate our rationalized transformer predic- tor against a variety of baseline methods that pur- suit different strategies for rationalizing predictions. This section provides a general overview, with many model and training details being discussed in Appendix A.2. The first group are rationalized neural networks, that learn to create rationales from sample-level labels alone. We evaluate the perfor- mance of the following methods 2-Player A two-player structure using a ra- tionale extractor and a predictor as proposed by Lei et al. (2016). We used an own imple- mentation, since the original work did not use transformers. 3-Player A three-player structure using a rationale extractor, a predictor and a comple- ment predictor as proposed by Yu et al. (2019). We used an own implementation, since the original work did not use transformers. CAR The CAR framework for creating class- wise rationales (Chang et al., 2019). We use an own implementation, since the original work did not use transformers. Additionally,",
    "metadata": {
      "document_id": 0,
      "chunk_id": 15,
      "source": "pdf_0",
      "chunk_length": 3704
    },
    "id": 15
  },
  {
    "text": "comple- ment predictor as proposed by Yu et al. (2019). We used an own implementation, since the original work did not use transformers. CAR The CAR framework for creating class- wise rationales (Chang et al., 2019). We use an own implementation, since the original work did not use transformers. Additionally, we use more extensive parameter sharing, as the original work use a separate rationale pre- dictor for each class, which is impracticable especially for the 10-class classification prob- lem on the INAS dataset. Therefore, a single BERT model predicts rationales for each class at the same time, while a second BERT model acts as the single predictor. A2R The A2R framework as proposed by (Yu et al., 2021). We use the implementa- tion of (Storek et al., 2023), who created 11904 an implementation relying on BERT models, which, according to their evaluation, outper- formed the original implementation that relies on GRUs. A2R-Noise The A2R framework with addi- tional noise injection as proposed by (Storek et al., 2023). We use the implementation pro- vided by the original study. We also evaluated a variety of post-hoc explana- tion methods MaRC The MaRC method as proposed by (Brinner and Zarrieß, 2023). We use the up- dated weight regularizer proposed by (Brinner et al., 2024). Occlusion The occlusion method as pro- posed by Zeiler and Fergus (2014). We chose to mask slightly larger spans of 5 tokens as this produced smoother masks which resulted in higher IoU F1 scores. We use the imple- mentation by Kokhlikyan et al. (2020). Int. Grads The integrated gradients method (Sundararajan et al., 2017). We use the imple- mentation by (Kokhlikyan et al., 2020). LIME The LIME method (Ribeiro et al., 2016). We train a linear classifier on scores from 50 function evaluations. In each eval- uation, 5 13 of tokens are selected and the thee tokens starting from the chosen token are removed as input perturbation. We use the implementation by (Kokhlikyan et al., 2020). Shapley Shapley value sampling (Castro et al., 2009). We perform 25 feature permuta- tions per sample, and use the implementation by (Kokhlikyan et al., 2020). L2E-MaRC The L2E framework (Situ et al., 2021). We use the rationales created by the MaRC method on the training samples. We discretize the rationales into 5 bins and train a classifier on this dataset. For scoring, we predict the bin-probabilities for each word, multiply them by the bin-means and sum over the resulting values to get a single, continuous score for each word. We also evaluate two further baselines A ran- dom baseline that predicts random scores for each input token, and a supervised method that is trained to perform a binary prediction on each individual token from the input. A.2 Model and Training Details Base Models For the movie review experiment, we use bert-base-uncased (Devlin et al., 2019) as base model to stay consistent with previous work and the be able to use existing code bases to ensure im- plementational accuracy. For the INAS dataset, we use PubMedBERT-base-uncased (Gu et al., 2021), since it shows strong performance on",
    "metadata": {
      "document_id": 0,
      "chunk_id": 16,
      "source": "pdf_0",
      "chunk_length": 3112
    },
    "id": 16
  },
  {
    "text": "movie review experiment, we use bert-base-uncased (Devlin et al., 2019) as base model to stay consistent with previous work and the be able to use existing code bases to ensure im- plementational accuracy. For the INAS dataset, we use PubMedBERT-base-uncased (Gu et al., 2021), since it shows strong performance on the standard classification task for this dataset (Brinner et al., 2022). These base classifiers are used for all base- line methods, and for all parts of the pipelines (en- coder, predictor, base classifier, etc.). Input Processing During training, samples that exceed the 510 token limit for BERT models were split into multiple segments, and one segment was chosen randomly for this model update. For the evaluation, we again split each sample into smaller parts that adhere to the token limit and that overlap for 100 tokens. Scores were predicted for each split separately and linearly blended afterwards. Model Selection During training, evaluations on the validation set were performed after each epoch, and the best-performing version of the model was selected for testing. On the INAS dataset, the agree- ment of the predicted rationales with the human annotations was evaluated after each epoch, and the mean of all five scores (AUC-PR, Token-F1, D- Token-F1, IoU-F1, D-IoU-F1) as well as the clas- sification performance was taken as performance indicator. On the movie dataset, this procedure was not possible, since the data distribution of the validation and test samples is different, meaning that validation results are not a good indicator for performance on the test set. Especially the span- level evaluation scores were unsuitable, since much shorter spans were annotated on the validation set. We chose to use the AUC-PR as performance mea- sure, since it still indicates the models ability to generally recognize useful words, which was again combined with classification f1 as performance in- dicator. A.3 RTP Objective Details We use two regularizers in the optimization objec- tive for our rationalized transformer predictor. The first is a sparsity regularizer that ensures that only 11905 a subset of tokens is selected as rationale Ωλ X cy α1 mean(mc)2 α2 mean(mc) X cy α3 mean(mc)2 α4 mean(mc) In summary, we perform L1 and L2 regularization on the mask means for the masks of the ground truth classes and non-ground-truth classes. In our experiments, we used α1 0.2 and α3 0.05, meaning that regularization for masks of incorrect classes is weaker. We chose this setting, since for incorrect classes there is no signal that forces words the be unmasked, so that less strong regularization is required. The L1 parameters are set to rather low values of α2 α4 0.001. The smoothness regularizer has the following form Ωσ β1 X cY mean((σc β2)2) Here, the inner subtraction is meant to be element- wise, so that we regularize each individual sigma value towards a value of β2. The actual hyperpa- rameters used are β1 0.02 and β2 3. Finally, we use individual weights for each major component of the optimization objective (Equation 8) arg min M γ1 CE(M(x), y) X cy γ2 Lc γ3 Lc γ4 Ωλ γ5",
    "metadata": {
      "document_id": 0,
      "chunk_id": 17,
      "source": "pdf_0",
      "chunk_length": 3126
    },
    "id": 17
  },
  {
    "text": "each individual sigma value towards a value of β2. The actual hyperpa- rameters used are β1 0.02 and β2 3. Finally, we use individual weights for each major component of the optimization objective (Equation 8) arg min M γ1 CE(M(x), y) X cy γ2 Lc γ3 Lc γ4 Ωλ γ5 Ωσ These values are set to γ1 2, γ2 5, γ3 5, γ4 3 and γ5 3. A.4 Evaluation Post-Processing In the INAS dataset, rationales do not cross sen- tence boundaries. For that reason, we opted to employ a post-processing step that uses SciSpacy (Neumann et al., 2019) to split each abstract into sentences, and set the rationale score of the last token in each sentence (that corresponds to punctu- ation) to 0. This is done for all methods and gen- erally lead to a slight improvement in agreement scores. 11906 B Examples B.1 Movie Reviews Dataset Figure 2 An exemplary output of the RTP for a positive review from the movie reviews dataset. Green text indicates the ground-truth annotations. Figure 3 An exemplary output of the RTP for a negative review from the movie reviews dataset. Green text indicates the ground-truth annotations. B.2 INAS Dataset Figure 4 An exemplary output of the RTP for an abstract by Jarnevich et al. (2006), which is included in the INAS dataset. The rationale was created for the Biotic Resistance Hypothesis label, with green spans indicating the ground-truth annotations. Figure 5 An exemplary output of the RTP for an abstract by Cripps et al. (2011), which is included in the INAS dataset. The rationale was created for the Enemy Release Hypothesis label, with green spans indicating the ground- truth annotations. 11907",
    "metadata": {
      "document_id": 0,
      "chunk_id": 18,
      "source": "pdf_0",
      "chunk_length": 1611
    },
    "id": 18
  },
  {
    "text": "Model Interpretability and Rationale Extraction by Input Mask Optimization Marc Brinner and Sina Zarrieß Bielefeld University Faculty for Linguistics and Literary Studies marc.brinner,sina.zarriessuni-bielefeld.de Abstract Concurrent with the rapid progress in neural network-based models in NLP, the need for cre- ating explanations for the predictions of these black-box models has risen steadily. Yet, espe- cially for complex inputs like texts or images, existing interpretability methods still struggle with deriving easily interpretable explanations that also accurately represent the basis for the models decision. To this end, we propose a new, model-agnostic method to generate extrac- tive explanations for predictions made by neu- ral networks, that is based on masking parts of the input which the model does not consider to be indicative of the respective class. The mask- ing is done using gradient-based optimization combined with a new regularization scheme that enforces sufficiency, comprehensiveness, and compactness of the generated explanation. Our method achieves state-of-the-art results in a challenging paragraph-level rationale extrac- tion task, showing that this task can be per- formed without training a specialized model. We further apply our method to image inputs and obtain high-quality explanations for image classifications, which indicates that the objec- tives for optimizing explanation masks in text generalize to inputs of other modalities. 1 Introduction Black-box machine-learning models like transform- ers (Vaswani et al., 2017) or convolutional neural networks (Tan and Le, 2019) are state-of-the-art in natural language processing and computer vi- sion. Their complexity enables them to perform well on a variety of tasks, but this comes at the cost of a lack of interpretability The question of why a model made a specific prediction cannot be answered reliably. Especially if such black-box models are used in critical real-world applications (e.g., in the medical domain), this creates a demand for methods that explain network predictions while fulfilling a variety of requirements, like being easy to implement, model and task agnostic, faithful to the inner workings of the network, and producing results that are easily interpretable for humans. To this end, a variety of interpretability methods have been proposed (Guidotti et al., 2018 Zhang et al., 2021), but as the aforementioned require- ments are often at odds, at least one of them often remains unfulfilled. Reasons for this include the reliance on complex message passing schemes that require laborious implementations (e.g., Montavon et al., 2017, Shrikumar et al., 2017), the applicabil- ity only to specific model architectures (e.g., Yuan et al., 2021, Abnar and Zuidema, 2020), or the fact that explanations often highlight individual, discon- nected input features (e.g., standard gradient-based saliency), which contradicts human intuition of a sensible explanation (compare Section 2.1 for de- tails). As an example, in a text classification setting, interpretability methods often highlight individual words that explain the prediction, but do not in- clude their context (Remmer, 2022), even though the context of a word is crucial in determining its meaning The word good influences the predic- tion in a completely different way if it is preceded by the word not, meaning that this context has an impact on the classification and should",
    "metadata": {
      "document_id": 1,
      "chunk_id": 0,
      "source": "pdf_1",
      "chunk_length": 3463
    },
    "id": 19
  },
  {
    "text": "clude their context (Remmer, 2022), even though the context of a word is crucial in determining its meaning The word good influences the predic- tion in a completely different way if it is preceded by the word not, meaning that this context has an impact on the classification and should therefore be part of the rationale. Notably, this holds true even in the absence of such modifiers, since the context must be available to confirm this absence. In this work, we propose a new method for model explainability that is able to identify parts of the input that are, on the one hand, most indicative of a class and, on the other hand, perceived as a sensible rationale by humans. Our method is appli- cable to all input types that define a spatial structure between individual features (e.g., texts, images) and builds on the assumption that interpretable ex- planations correspond to smooth and connected regions of features with respect to this spatial struc- ture. It uses numerical optimization to mask out parts of the input that the model does not consider arXiv2508.11388v1 cs.CL 15 Aug 2025 indicative of the class of interest, thus leaving only the parts of the input that are indicative of this class. The masking is done using gradient-based optimiza- tion combined with a new regularization scheme that enforces sufficiency, comprehensiveness, and compactness of the generated explanation (Yu et al., 2019), three criteria that have been established in the domain of rationale extraction but are less com- mon in network interpretability methods. In this way, our method bridges the gap between model interpretability and rationale extraction, thereby showing that the latter of which can be performed without training a specialized model, only on the basis of a trained classifier. 2 Background Methods that explain the predictions made by black-box models to users can be broadly catego- rized into (i) interpretability methods that aim at creating explanations for existing classifiers after they have been trained (Section 2.1) and (ii) ra- tionale extraction approaches that are designed to create a rationale as a model output in addition to the usual label prediction (Section 2.2). Our inter- pretability method relies on gradient-based input optimization, discussed in detail in Section 2.3. 2.1 Neural Network Interpretability Interpretability methods usually assign importance scores to features or parts of a given input, in- dicating how relevant the respective feature is for making the prediction. Many early meth- ods focus on convolutional neural networks and use backpropagation-like procedures to compute saliency scores for each input feature. Simonyan et al. (2013) use the networks gradient at the in- put image as saliency scores, while Sundararajan et al. (2017)s integrated gradients method sums over gradients at different inputs that are created by gradually transforming a neutral input into the in- put of interest. The DeconvNet architecture (Zeiler and Fergus, 2014) and the guided backpropagation algorithm (Springenberg et al., 2015) again rely on a single evaluation but change the standard gradient computation to produce visually improved impor- tance maps. Attribution methods like layer-wise rel- evance propagation Bach et al. (2015)",
    "metadata": {
      "document_id": 1,
      "chunk_id": 1,
      "source": "pdf_1",
      "chunk_length": 3283
    },
    "id": 20
  },
  {
    "text": "in- put of interest. The DeconvNet architecture (Zeiler and Fergus, 2014) and the guided backpropagation algorithm (Springenberg et al., 2015) again rely on a single evaluation but change the standard gradient computation to produce visually improved impor- tance maps. Attribution methods like layer-wise rel- evance propagation Bach et al. (2015) extend this idea by defining a backward pass that redistributes the total function value layer-wise backwards using a propagation rule that makes the total relevancy within each layer add up to the function value that is to be explained. Deep Taylor Decomposition (Mon- tavon et al., 2017) and DeepLIFT (Shrikumar et al., 2017) then introduced different rules for redistribut- ing the relevance between layers. For transformer models, methods like (Abnar and Zuidema, 2020) track the attention flow through the network. This has been extended to incorporate information from attribution methods like the Deep Taylor Decompo- sition to more accurately identify neurons that have a strong influence on the final prediction (Chefer et al., 2021b,a). Other well-known explainability methods rely on input perturbations. LIME (Ribeiro et al., 2016) identifies important input features by perturbing the input, observing the change in the model pre- dictions, and fitting an interpretable model to the observed data, while other methods occlude parts of the input to detect features that are important for the classification (Zeiler and Fergus, 2014 Bazzani et al., 2016 Zhou et al., 2014 Petsiuk et al., 2018). A further approach to model interpretability is to generate an input that maximally activates specific neurons, thereby yielding insights about the respon- sibilities of these neurons, as was done for CNNs by Simonyan et al. (2013). Fong and Vedaldi (2017) then used a similar idea to remove class-indicative information from input images to detect the parts of the image responsible for the classification. The method we propose in this paper differs from standard gradient-based techniques by not re- lying on evaluations at a single point or at fixed perturbations, but at points that are determined by a dynamic optimization process. This control via optimization is also a key difference from meth- ods that rely on random permutations or masking of input features. Compared to message-passing schemes and model-specific methods (like methods for transformer interpretability), relying only on the gradient makes our method applicable to models with a variety of architectures and layer types with- out requiring additional implementational effort. 2.2 Rationale Extraction The task of rationale extraction, also commonly re- ferred to as selective rationalization, is concerned with designing models that can produce human- interpretable rationales in addition to the usual model output (Lei et al., 2016), with the domain usually being textual inputs and the rationales be- ing a subset of the input text that is determined to be responsible for the prediction. Lei et al. (2016) approached this task by developing a two- step procedure in which a proposal network ex- tracts a rationale from the input text and a subse- quent classification network only has access to the rationale to make the final prediction. By training this model end-to-end, the proposal network learns to",
    "metadata": {
      "document_id": 1,
      "chunk_id": 2,
      "source": "pdf_1",
      "chunk_length": 3340
    },
    "id": 21
  },
  {
    "text": "(2016) approached this task by developing a two- step procedure in which a proposal network ex- tracts a rationale from the input text and a subse- quent classification network only has access to the rationale to make the final prediction. By training this model end-to-end, the proposal network learns to extract the most useful text fragments from the input, which thus corresponds to an explanation for the classification. Later, Yu et al. (2019) proposed three criteria that rationales should satisfy to be perceived as sensible Sufficiency The rationale should be sufficient to correctly classify the sample only by its ratio- nale. Comprehensiveness All relevant information should be contained in the rationale, mean- ing that the correct label can not be inferred by just considering the words not included in the rationale. Compactness The rationale should be sparse but should nevertheless consist of consecutive text fragments instead of single words. Yu et al. (2019)s methods enforce these criteria through regularizers and by using a complement predictor that predicts the correct label based on all words that are not part of the rationale. Train- ing the proposal network to fool the complement predictor then enforces the comprehensiveness con- straint. Other approaches extend this and extract class-dependent rationales (Chang et al., 2019) or select complete paragraphs as rationales (Chalkidis et al., 2021). The two main differentiators of rationale extrac- tion models to the interpretability methods dis- cussed in Section 2.1 are that, one the one hand, models are explicitly trained to produce rationales instead of creating them post hoc, and, on the other hand, the focus is on creating human interpretable rationales while the focus for interpretability meth- ods often is on mathematical faithfulness measures. Our method combines the focus on faithfulness with the desire for human interpretability to create rationales that faithfully explain model predictions post hoc and correspond to human rationales, as these properties substantially enhance the useful- ness of explanations for many applications. 2.3 Input Optimization As mentioned in Section 2.2, optimization of input images for CNNs has been used to explain the re- sponsibilities of specific neurons, but notably, the resulting images do not resemble naturally occur- ring images. This is caused by the huge complexity and highly nonlinear behavior of neural networks, leading to the property of having unpredictable be- havior on out-of-domain inputs that quickly arise during the optimization. In different experiments, this has led to behaviors like making highly con- fident class predictions for images that resemble random noise (Nguyen et al., 2015) or predicting a completely different class after adding almost im- perceivable noise to a given image (Szegedy et al., 2014). Unconstrained optimization of the input to a neural network to optimize the activation of specific neurons will therefore inevitably result in inputs that are out-of-domain, do not resemble nat- ural images, or seem downright counter-intuitive. Different strategies for mitigating this problem in the context of input optimization exist (e.g., the use of GANs, Nguyen et al., 2016), with the most com- mon being extensive regularization to prevent high- frequency information in images from influencing the",
    "metadata": {
      "document_id": 1,
      "chunk_id": 3,
      "source": "pdf_1",
      "chunk_length": 3383
    },
    "id": 22
  },
  {
    "text": "do not resemble nat- ural images, or seem downright counter-intuitive. Different strategies for mitigating this problem in the context of input optimization exist (e.g., the use of GANs, Nguyen et al., 2016), with the most com- mon being extensive regularization to prevent high- frequency information in images from influencing the prediction (Yosinski et al., 2015 Mahendran and Vedaldi, 2015) or using lower-resolution inputs and blurring to limit the degrees of freedom within the optimization (Fong and Vedaldi, 2017). In this study, we use input optimization to per- form model interpretability by optimizing a mask to suppress all parts of a given input that a given model does not consider indicative of the given class. Compared to (Fong and Vedaldi, 2017), we propose a new optimization objective as well as a new regularization scheme that allows for the creation of more detailed masks. Additionally, we expand the scope of input optimization methods from the domain of images to text processing. 3 MaRC In this section, we introduce MaRC, our frame- work for Mask-based Rationale Creation. Section 3.1 develops the general framework. Sections 3.2 and 3.3 address the specificities of applying MaRC to texts and images, respectively. 3.1 Method We design an interpretability method that detects parts of an input x that a model M considers most indicative of a specific class c. We assume an input x with n input features, each of which could be high-dimensional, e.g., token embeddings or pixels with color channels. The main idea of the approach is to detect input features that are highly indicative of class c by replacing as much of the input as pos- sible with an uninformative input b, i.e., an input that the model does not consider indicative of any class, while having the model assign a high score for class c to the altered input. We define a mask λ Rn, λi 0, 1 to obtain a masked input x in the following way x λ x (1 λ) b (1) When λi is close to 1, feature i is mostly retained in x while λi values close to 0 replace feature i almost completely with the uninformative b. MaRC tackles this masking as an optimization problem it optimizes λ to obtain rationales that fulfill the properties of sufficiency, comprehensive- ness, and compactness (compare Section 2.2). It models these properties via dedicated regularizers, which we will develop step by step in the following. Sufficiency We want to find a mask λ such that the probability that model M assigns to x for class c is close to 1. We optimize this criterion as fol- lows arg min λ0,1n L(x, c) αλ 1 n n X i1 λi 2 z Ωλ (2) Here, L(x, c) is a scoring function for c under M and Ωλ is a sparsity regularizer that enforces the detection of the smallest set of input features that still induces a high score for c. An obvious choice for L(x, c) is the log-likelihood of c, maximiz- ing the probability of c under M and leading λ to highlight class-discriminative",
    "metadata": {
      "document_id": 1,
      "chunk_id": 4,
      "source": "pdf_1",
      "chunk_length": 2940
    },
    "id": 23
  },
  {
    "text": "and Ωλ is a sparsity regularizer that enforces the detection of the smallest set of input features that still induces a high score for c. An obvious choice for L(x, c) is the log-likelihood of c, maximiz- ing the probability of c under M and leading λ to highlight class-discriminative information, i.e., input features that indicate only class c. A different choice would be the logarithm of the sigmoid of the logit for c, which does not suppress other classes and therefore leads λ to highlight class-indicative information, i.e., all input features relevant for c, even if they are indicative of other classes as well. In both cases, M considers x to be highly indica- tive of class c, thereby fulfilling the sufficiency criterion. Comprehensiveness Optimizing Equation 2 leads to sufficiency but not comprehensiveness, as the smallest set of highly indicative input features is detected. To detect all information relevant for c, we introduce the complement of rationale (Yu et al., 2019) xc (1 λ) x λ b (3) which leaves features unmasked that were masked for x. Minimizing the score of xc for c enforces all parts that indicate class c to be masked in xc (mean- ing that they will be unmasked in x), resulting in the following optimization arg min λ0,1n L(x, c) L(xc, c) Ωλ (4) This formulation combines the deletion game and preservation game that were introduced by Fong and Vedaldi (2017) but treated as separate objectives. Optimizing the mask with respect to both objectives greatly supports the detection of precise boundaries of the relevant features. Compactness The original compactness crite- rion states that a rationale shall consist of longer but fewer meaningful spans of text. Here, we gen- eralize this to all input types that possess a spatial structure that defines neighborhoods around input variables. The underlying assumption is, that for these types of inputs, a feature is only meaningful in the context of its neighborhood, as, for example, is the case for single words in text or individual pixels in images, so that a sensible rationale must include larger groups of closely located features. Thus, we now assume a general spatial structure on the input x that defines distances d(i, j) between the features i and j, with features that are closer together having a higher chance of belonging to the same meaningful entity. We enforce the selection of larger groups of features by reparameterizing our mask, i.e., we introduce two new parameters, w Rn and σ Rn 0 from which the mask values λ can subsequently be calculated. The optimization is then performed with respect to w and σ. The mask values λ are mainly determined by w, in a way that wi largely determines the final value of λi. Crucially, wi now also influences the values of λ around i, so that, for example, λi1 and λi1 are also strongly influenced by wi. σi then determines the strength and extent of wis influence on its neighbors, as it parameterizes an unnormalized Gaussian placed at position i, so that the influence wij of a",
    "metadata": {
      "document_id": 1,
      "chunk_id": 5,
      "source": "pdf_1",
      "chunk_length": 3026
    },
    "id": 24
  },
  {
    "text": "influences the values of λ around i, so that, for example, λi1 and λi1 are also strongly influenced by wi. σi then determines the strength and extent of wis influence on its neighbors, as it parameterizes an unnormalized Gaussian placed at position i, so that the influence wij of a weight wi onto λj is then given by wij wi exp d(i, j)2 σi (5) Figure 1 An exemplary rationale created by MaRC for the prediction of the positive sentiment label. The final value for λj is then calculated as follows λj sigmoid( X i wij) (6) This parameterization of λ enforces neighboring inputs to have similar values if the corresponding σ values are large, which also plays a key role in regularizing the optimization to avoid the issues dis- cussed in Section 2.3. Large values for σ are softly enforced by introducing an additional regularizer Ωσ ασ 1 n n X i1 log(σi) (7) The logarithm was chosen to enforce positive val- ues of σi while gradually discounting the effect that increases in σi have on the loss function. Notably, this regularizer does not enforce large values of σ by means of hard constraints, meaning that low values and therefore sharper boundaries between mask values for neighboring features can be opti- mal if the other parts of the optimization objective support this behavior. This is in contrast to (Fong and Vedaldi, 2017), who used a lower resolution mask in combination with upsampling and Gaus- sian blur to detect smooth masks, which does not allow for sharp masks even if they were optimal. In summary, the final optimization objective looks as follows arg min w,σRn L(x, c) L(xc, c) Ωλ Ωσ (8) This objective can be optimized using stochastic gradient descent, but in practice, we found using an optimizer that incorporates momentum (e.g., Adam, Kingma and Ba, 2015) to be key for avoiding local optima and obtaining optimal results. 3.2 Textual Inputs As MaRC only requires the gradient of a model prediction at the input, it can be applied to all com- mon text processing models. In the following, we discuss specific aspects of using MaRC with state- of-the-art transformer architectures like BERT (De- vlin et al., 2019). As uninformative input b, we choose a sequence of PAD-tokens of the same length as x. During training, the model learns to treat these tokens as uninformative since they are added to inputs irre- spective of their content or the desired output. As we want importance scores for each individ- ual word, we define n to be the number of words in the input sequence. Notably, this is different from the actual input dimension, as it is common to use WordPiece embeddings (Wu et al., 2016) which could split words into multiple input tokens. In this case, we use parameter tying to only have a single parameter for all pieces of a word represen- tation. The distance function is then simply defined as d(i, j) ij, with i and j being the positions of the words in the text. Finally, we found that introducing",
    "metadata": {
      "document_id": 1,
      "chunk_id": 6,
      "source": "pdf_1",
      "chunk_length": 2947
    },
    "id": 25
  },
  {
    "text": "In this case, we use parameter tying to only have a single parameter for all pieces of a word represen- tation. The distance function is then simply defined as d(i, j) ij, with i and j being the positions of the words in the text. Finally, we found that introducing noise into the optimization process is beneficial for regularization (see Section 2.3 for discussion of regularization in input optimization). Thus, for text inputs, we add Gaussian noise to x and xc and randomly set mask values to 0 or to 1 in each optimization step. 3.3 Image Inputs Image inputs also fulfill the requirements on the presence of a spatial structure that is needed for our method. They also provide natural choices for unin- formative inputs, as uniformly colored images can generally be assumed to be uninformative in most prediction settings. Therefore, obvious choices for b would, for example, be a white image, a black image, or an image of the mean color within the given dataset. A different option is to remove us- able information from the input image by blurring it and using this blurred image as uninformative in- put (Fong and Vedaldi, 2017). As parts of the input image could have the same color as the uninforma- Method Token F1 mAP IoU F1 Suff.Comp. MaRC .473 .469 .163 .028 .518 Occlusion .432 .448 .125 .022 .415 Saliencyn .435 .392 .04 .132 .287 Saliencys .425 .340 .076 .260 .246 InXGradn .436 .396 .040 .136 .292 InXGrads .425 .340 .084 .239 .248 Int. Gradsn .428 .369 .036 .122 .274 Int. Gradss .431 .381 .071 .048 .528 LIME .436 .380 .076 .047 .496 Shapley .428 .439 .079 -.015 .728 Noise .454 .450 .139 .034 .487 Ωλ .349 .350 .046 .120 .266 Ωσ .447 .425 .091 .036 .535 L(xc, c) .396 .436 .123 .052 .304 Table 1 Results on rationale extraction on the movie reviews dataset (DeYoung et al., 2020), including faith- fulness evaluation. See Section A for an overview of the methods tested and for experimental details. tive input (which renders the corresponding mask values meaningless) and even uniformly colored patches could be seen as informative by neural net- works, we chose to alter the optimization objective to be the average over different choices for b, with B being the set of all uninformative inputs arg min w,σRwh 1 B X bB L(x(b, τ), c) L(xc(b, τ), c) Ωλ Ωσ ΩNB (9) As images generally have more variables and there- fore more degrees of freedom in the optimization, further regularization is needed to obtain sensible optimization results. To this end, this formulation includes an additional regularizer ΩNB, which de- notes the average squared difference between mask values that are neighboring with respect to the 8- connected grid structure of the image, weighted by a corresponding parameter αNB. To complete the specification of the optimiza- tion problem, we define the distance function d between two pixels to be the euclidean distance between their two-dimensional position vectors in the image grid. In contrast to the textual inputs, the introduction of noise to the optimization",
    "metadata": {
      "document_id": 1,
      "chunk_id": 7,
      "source": "pdf_1",
      "chunk_length": 3014
    },
    "id": 26
  },
  {
    "text": "by a corresponding parameter αNB. To complete the specification of the optimiza- tion problem, we define the distance function d between two pixels to be the euclidean distance between their two-dimensional position vectors in the image grid. In contrast to the textual inputs, the introduction of noise to the optimization process did not prove to be beneficial. ResNet-101 ViT-B16 Method Suff. Comp. Suff. Comp. MaRC .196 .612 .139 .596 M-Perturb .260 .605 .174 .572 Grad-CAM .197 .600 .161 .640 Exc-BP .302 .600 - - Saliency .442 .599 .355 .528 InputXGrad .430 .586 .366 .506 Guided BP .343 .630 - - Intgr. Grads .344 .641 .261 .641 Occlusion .324 .606 .194 .486 Attention - - .241 .562 Attribution - - .176 .608 Rollout - - .205 .580 TAM - - .146 .658 Table 2 Results for the faithfulness evaluation of dif- ferent explainability methods for ResNet-101 and ViT- B16. Compare Section A for an overview of the meth- ods tested and for experimental details. 4 Experiments on Rationale Extraction We evaluate MaRC on rationale extraction, a task that is concerned with predicting the correct label for a given textual input while also providing a subset of the input as a rationale for the prediction. 4.1 Data We use the movie review data set (Zaidan et al., 2007) with 2000 movie reviews annotated with sentiment labels (positive or negative) as well as span-level rationales. We test on the additional ra- tionales created by DeYoung et al. (2020), which are more comprehensive and thus, on average, com- prise a much larger fraction of words (7.2 vs. 31.4). As our approach is designed for extracting span-level rationales and most other datasets for rationale extraction are not annotated on span-level (DeYoung et al., 2020), this is the only dataset suit- able for an evaluation of MaRC. We use a standard BERTbase model (Devlin et al., 2019) and train it as a standard binary classification model on the training data, therefore only using the class labels and not the annotated rationales. 4.2 Evaluation There are two common ways of evaluating the ra- tionales produced by different models (DeYoung et al., 2020 Atanasova et al., 2020) (a) Input (b) MaRC (c) M-Perturb (d) Occlusion (e) Integr. Grads (f) Grad-CAM Figure 2 Comparison of masks created for ResNet-101 by different explainability methods. 1. Agreement with human annotator ratio- nales A strong overlap between rationales given by human annotators and rationales pro- duced by an explainability model is a good indicator that sensible rationales have been selected. Additionally, similarity to human ra- tionales could be considered a desirable prop- erty (depending on the use case), even if it is not perfectly in line with the actual reasoning process of the neural network. 2. Faithfulness Ideally, the rationales produced by a model fulfill the conditions of sufficiency and comprehensiveness, meaning that they ac- tually reveal the information that the model considered indicative for the predicted label. Different metrics exist to evaluate the performance of approaches that produce soft scores (i.e., con- tinuous values) or binary values as the output of the rationale",
    "metadata": {
      "document_id": 1,
      "chunk_id": 8,
      "source": "pdf_1",
      "chunk_length": 3153
    },
    "id": 27
  },
  {
    "text": "fulfill the conditions of sufficiency and comprehensiveness, meaning that they ac- tually reveal the information that the model considered indicative for the predicted label. Different metrics exist to evaluate the performance of approaches that produce soft scores (i.e., con- tinuous values) or binary values as the output of the rationale generation. As we see use cases for both outputs, we evaluate our approach with respect to both. To create a binary mask from the continuous mask values that MaRC produces, we train a kernel regression model to predict the optimal percentage of words that need to be included in the rationale (described in Appendix A), which we do in the same way for all methods tested in this study. To evaluate the agreement with human ratio- nales, we calculate the token F1 score for the bi- nary masks by using precision and recall of the positive class of words belonging to the ratio- nale, while the soft-scoring models are evaluated using the mean average precision (mAP). To evalu- ate the agreement of larger detected spans with the spans present in the human rationales, we evaluate the IoU F1 score that counts a ground-truth span as correctly detected if there is a predicted span with an IoU of over 0.5, which again allows for the calculation of an F1 score for the positive class of detecting the spans. These three metrics were used in the ERASER benchmark (DeYoung et al., 2020), which also proposed metrics to evaluate suf- ficiency and comprehensiveness. For these metrics, we slightly deviate from their evaluation metrics by evaluating these scores for a given sample x and rationale r in the following way comp(x, r) 1 19 19 X i1 M(x) M(xri) (10) sufficiency(x, r) 1 19 19 X i1 M(x) M(ri) (11) Here, M(x) denotes the class probability predic- tion (for the ground-truth class) of our model, ri de- notes the top (i5) of words according to the soft rationale scores (all other words are removed), and xri denotes sample x with all words that belong to ri removed, where we remove words by re- placing the corresponding tokens with PAD-tokens. Therefore, the comprehensiveness score evaluates, how much removing the rationale decreases the model performance (higher scores are better) while the sufficiency score evaluates how well the cor- rect label can be predicted from the rationale alone (lower scores are better). 4.3 Results The evaluation results are displayed in the upper part of Table 1 (see Appendix A for more details on the setup). We compare MaRC against other interpretability methods that are commonly used in the context of NLP but omit specialized ratio- nale extraction models as they (i) usually produce binary masks, making it impossible to perform the soft-scoring evaluation, and (ii) do not produce ex- planations for existing models, which makes the faithfulness evaluation inapplicable. We see that MaRC achieves state-of-the-art re- sults on all measures that evaluate agreement with human rationales, i.e. Token F1, mAP, and IoU F1, showing that MaRC is the best method for obtaining rationales that match human intuition. Especially with",
    "metadata": {
      "document_id": 1,
      "chunk_id": 9,
      "source": "pdf_1",
      "chunk_length": 3117
    },
    "id": 28
  },
  {
    "text": "planations for existing models, which makes the faithfulness evaluation inapplicable. We see that MaRC achieves state-of-the-art re- sults on all measures that evaluate agreement with human rationales, i.e. Token F1, mAP, and IoU F1, showing that MaRC is the best method for obtaining rationales that match human intuition. Especially with respect to the IoU F1 score, MaRC outperforms all other methods by a large margin, even though the hyperparameters for other meth- ods were set to explicitly support high scores in this measure (e.g., masking larger spans for occlusion (a) Input (b) MaRC Softmax (c) MaRC Sigmoid (d) M-Perturb (e) TAM (f) Grad-CAM Figure 3 Comparison of masks created for ViT-B16 by different explainability methods. and LIME). This highlights that MaRC is suitable for detecting span-level rationales in a paragraph- long text that agree with spans that humans anno- tate, without being trained to do so and without additional model components as in state-of-the-art rationale extraction models. For sufficiency and comprehensiveness, MaRC also achieves impressive results, being outper- formed in both metrics only by Shapley value sam- pling. The excellent performance of this method with regard to these evaluation metrics is not sur- prising, though, as it is based on choosing a random permutation of input features, adding them succes- sively to the input, and using the change in the models output as the resulting score. This method is very closely connected to the sufficiency and comprehensiveness calculations, thereby rendering the great results of this method unsurprising. It should be noted, that multiple methods, including MaRC, achieve close to optimal results for suffi- ciency, as scores close to 0 indicate that the removal of very few high-scoring tokens is enough to com- pletely throw off the classifier. Notably, MaRC can produce good results while aiming to create human-like rationales, showing that this kind of rationale to some extent corresponds to the inner workings of the neural network. We also conduct an ablation study that tests the importance of the different parts of the optimization objective by leaving these parts out in turn and reporting the results with the altered objective. The results are displayed in the lower part of Table 1, with the Method column specifying with part of the objective is omitted. The full optimization objective is almost uniformly the best-performing variant, proving that all parts a essential to achieve optimal performance. 5 Experiments on Image Classification We evaluate MaRC on the task of creating ratio- nales for classifications of ImageNet (Russakovsky et al., 2015) images. A visual comparison of masks created by MaRC and other interpretabil- ity approaches for ResNet-101 (He et al., 2016) and the vision transformer ViT-B16 (Kolesnikov et al., 2021) is displayed in Figures 2 and 3, re- spectively (see Appendix C for with more visual- izations). We see that MaRC is able to produce sharp masks that often cover the complete object of interest in the image. For ViT-B16, we include a visualization highlighting the distinction between class-discriminative vs. class-indicative informa- tion (compare Section 3.1) Figure 3b) used the softmax of the model output as scoring",
    "metadata": {
      "document_id": 1,
      "chunk_id": 10,
      "source": "pdf_1",
      "chunk_length": 3285
    },
    "id": 29
  },
  {
    "text": "We see that MaRC is able to produce sharp masks that often cover the complete object of interest in the image. For ViT-B16, we include a visualization highlighting the distinction between class-discriminative vs. class-indicative informa- tion (compare Section 3.1) Figure 3b) used the softmax of the model output as scoring function, which leads MaRC to highlight only the head and tail of the animal, the two parts that the model uses to differentiate the correct class from the other classes. For Figure 3c), on the other hand, a mix- ture of the sigmoid of the class logit and the soft- max of the model output was used with a ratio of 91, making the model highlight all parts in the im- age that indicate the ground-truth class, as long as they are not significant indicators of other classes. We also evaluate the faithfulness of the expla- nation created by MaRC and a variety of other interpretability methods using the same metrics as for textual inputs. For this experiment, we use pretrained ResNet-101 and ViT-B16 models on a random sample of 500 ImageNet validation im- ages, with further implementational details being described in Appendix A. As shown in Table 2, MaRC is the best-performing model with respect to sufficiency for both ResNet-101 and ViT-B16, showing that the areas that MaRC highlights are indeed the areas that allow the model to predict the correct class based solely on these regions. With re- spect to comprehensiveness, MaRC achieves com- petitive results, only falling behind model-specific architectures that heavily use the knowledge about the inner workings of the model and the informa- tion flow inside it (e.g., transition attention maps (TAM), Yuan et al., 2021), as well as two other methods in the form of Guided Backpropagation (Springenberg et al., 2015) and Integrated Gradi- ents Sundararajan et al. (2017). The latter two methods often predict individual pixels that are spread over many areas of the image as the most in- dicative input features, indicating that the removal of key pixels at different positions of the image is a good strategy to quickly decrease the classifier performance, an approach that MaRC is actively discouraged to pursuit. 6 Conclusion We propose a new method for creating explanations for neural network predictions that are faithful to the models reasoning process as well as being sen- sible with respect to human judgment. We achieve state-of-the-art results on the task of rationale ex- traction, achieve competitive or state-of-the-art re- sults with respect to faithfulness, and provide vi- sually sensible explanations for classifications of images. As MaRC is model-agnostic, we believe it to be a useful tool in many areas of machine learning that include textual or image inputs. We further believe that other domains can make use of MaRC, including multimodal tasks that, for exam- ple, combine textual and image inputs, as well as other domains that fulfill the requirements on the spatial structure of the input, like auditory data. 7 Limitations Compared to other interpretability methods, MaRC is able to create explanations that more closely resemble human rationales. Nevertheless, the",
    "metadata": {
      "document_id": 1,
      "chunk_id": 11,
      "source": "pdf_1",
      "chunk_length": 3183
    },
    "id": 30
  },
  {
    "text": "tasks that, for exam- ple, combine textual and image inputs, as well as other domains that fulfill the requirements on the spatial structure of the input, like auditory data. 7 Limitations Compared to other interpretability methods, MaRC is able to create explanations that more closely resemble human rationales. Nevertheless, the simi- larity to human rationales is always limited by the inner workings of the respective neural network If a networks reasoning does not mirror human reasoning, the resulting rationales will be incom- prehensible to humans. Additionally, rationales created by MaRC are the result of a complete input optimization process. Therefore, the rationale creation usually requires hundreds of forward passes and gradient evalu- ations for the respective neural network, which makes the process of creating the rationale time- consuming and therefore infeasible for many real- time applications. On modern hardware, creating a rationale for BERTbase can take two to three min- utes depending on the length of the input text, while ResNet-101 and ViT-B16 are faster at about one minute. References Samira Abnar and Willem Zuidema. 2020. Quantify- ing attention flow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 41904197, On- line. Association for Computational Linguistics. Pepa Atanasova, Jakob Grue Simonsen, Christina Li- oma, and Isabelle Augenstein. 2020. A diagnostic study of explainability techniques for text classifi- cation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 32563274, Online. Association for Computational Linguistics. Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wo- jciech Samek. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise rele- vance propagation. PLOS ONE, 10(7)146. Loris Bazzani, Alessandra Bergamo, Dragomir Anguelov, and Lorenzo Torresani. 2016. Self-taught object localization with deep networks. In 2016 IEEE Winter Conference on Applications of Computer Vi- sion (WACV), pages 19. Javier Castro, Daniel Gómez, and Juan Tejada. 2009. Polynomial calculation of the shapley value based on sampling. Computers Operations Research, 36(5)17261730. Selected papers presented at the Tenth International Symposium on Locational Deci- sions (ISOLDE X). Ilias Chalkidis, Manos Fergadiotis, Dimitrios Tsarapat- sanis, Nikolaos Aletras, Ion Androutsopoulos, and Prodromos Malakasiotis. 2021. Paragraph-level ratio- nale extraction through regularization A case study on European court of human rights cases. In Pro- ceedings of the 2021 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics Human Language Technologies, pages 226241, Online. Association for Computational Lin- guistics. Shiyu Chang, Yang Zhang, Mo Yu, and Tommi Jaakkola. 2019. A game theoretic approach to class-wise se- lective rationalization. In Advances in Neural In- formation Processing Systems, volume 32. Curran Associates, Inc. Hila Chefer, Shir Gur, and Lior Wolf. 2021a. Generic attention-model explainability for interpreting bi- modal and encoder-decoder transformers. In 2021 IEEECVF International Conference on Computer Vision (ICCV), pages 387396. Hila Chefer, Shir Gur, and Lior Wolf. 2021b. Trans- former interpretability beyond attention visualization. In 2021 IEEECVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 782791. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT Pre-training of deep bidirectional transformers for language under- standing. In Proceedings",
    "metadata": {
      "document_id": 1,
      "chunk_id": 12,
      "source": "pdf_1",
      "chunk_length": 3707
    },
    "id": 31
  },
  {
    "text": "387396. Hila Chefer, Shir Gur, and Lior Wolf. 2021b. Trans- former interpretability beyond attention visualization. In 2021 IEEECVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 782791. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. 2020. ERASER A benchmark to evaluate rationalized NLP models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 44434458, Online. Association for Computational Linguistics. Ruth C. Fong and Andrea Vedaldi. 2017. Interpretable explanations of black boxes by meaningful pertur- bation. In 2017 IEEE International Conference on Computer Vision (ICCV). IEEE. Jacob Gildenblat and contributors. 2021. Pytorch li- brary for cam methods. httpsgithub.com jacobgilpytorch-grad-cam. Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018. A survey of methods for explaining black box models. ACM Comput. Surv., 51(5). Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recogni- tion. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770778. Diederik P. Kingma and Jimmy Ba. 2015. Adam A method for stochastic optimization. In 3rd Inter- national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz-Richardson. 2020. Captum A unified and generic model interpretability library for pytorch. Alexander Kolesnikov, Alexey Dosovitskiy, Dirk Weis- senborn, Georg Heigold, Jakob Uszkoreit, Lucas Beyer, Matthias Minderer, Mostafa Dehghani, Neil Houlsby, Sylvain Gelly, Thomas Unterthiner, and Xi- aohua Zhai. 2021. An image is worth 16x16 words Transformers for image recognition at scale. Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing neural predictions. In Proceedings of the 2016 Conference on Empirical Methods in Nat- ural Language Processing, pages 107117, Austin, Texas. Association for Computational Linguistics. Aravindh Mahendran and Andrea Vedaldi. 2015. Vi- sualizing deep convolutional neural networks using natural pre-images. International Journal of Com- puter Vision, 120233255. Grégoire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert Müller. 2017. Explaining nonlinear classification decisions with deep taylor decomposition. Pattern Recognition, 65211222. Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, and Jeff Clune. 2016. Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. In Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc. Anh Nguyen, Jason Yosinski, and Jeff Clune. 2015. Deep neural networks are easily fooled High con- fidence predictions for unrecognizable images. In 2015 IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR), pages 427436. Vitali Petsiuk, Abir Das, and Kate Saenko. 2018. Rise Randomized input sampling for explanation of black- box models. In British Machine Vision Conference. Eliott Remmer. 2022. Explainability methods for transformer-based",
    "metadata": {
      "document_id": 1,
      "chunk_id": 13,
      "source": "pdf_1",
      "chunk_length": 3805
    },
    "id": 32
  },
  {
    "text": "con- fidence predictions for unrecognizable images. In 2015 IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR), pages 427436. Vitali Petsiuk, Abir Das, and Kate Saenko. 2018. Rise Randomized input sampling for explanation of black- box models. In British Machine Vision Conference. Eliott Remmer. 2022. Explainability methods for transformer-based artificial neural networks a com- parative analysis. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. why should i trust you? Explain- ing the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 16, page 11351144, New York, NY, USA. Association for Computing Machinery. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexan- der C. Berg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3)211252. Ramprasaath R. Selvaraju, Michael Cogswell, Ab- hishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam Visual explana- tions from deep networks via gradient-based local- ization. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 618626. Avanti Shrikumar, Peyton Greenside, and Anshul Kun- daje. 2017. Learning important features through propagating activation differences. In Proceedings of the 34th International Conference on Machine Learn- ing, volume 70 of Proceedings of Machine Learning Research, pages 31453153. PMLR. Karen Simonyan, Andrea Vedaldi, and Andrew Zis- serman. 2013. Deep inside convolutional networks Visualising image classification models and saliency maps. CoRR, abs1312.6034. Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin A. Riedmiller. 2015. Striving for simplicity The all convolutional net. In 3rd Inter- national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings. Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In Proceed- ings of the 34th International Conference on Machine Learning - Volume 70, ICML17, page 33193328. JMLR.org. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks. In 2nd International Conference on Learn- ing Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings. Mingxing Tan and Quoc Le. 2019. EfficientNet Re- thinking model scaling for convolutional neural net- works. In Proceedings of the 36th International Con- ference on Machine Learning, volume 97 of Pro- ceedings of Machine Learning Research, pages 6105 6114. PMLR. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, volume 30. Curran Associates, Inc. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Googles neural machine translation system Bridging the gap between human and machine trans- lation.",
    "metadata": {
      "document_id": 1,
      "chunk_id": 14,
      "source": "pdf_1",
      "chunk_length": 3640
    },
    "id": 33
  },
  {
    "text": "Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Googles neural machine translation system Bridging the gap between human and machine trans- lation. CoRR, abs1609.08144. Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. 2015. Understanding neural net- works through deep visualization. In Deep Learn- ing Workshop, International Conference on Machine Learning (ICML). Mo Yu, Shiyu Chang, Yang Zhang, and Tommi Jaakkola. 2019. Rethinking cooperative rationalization In- trospective extraction and complement control. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 4094 4103, Hong Kong, China. Association for Computa- tional Linguistics. Tingyi Yuan, Xuhong Li, Haoyi Xiong, Hui Cao, and Dejing Dou. 2021. Explaining information flow in- side vision transformers using markov chain. In eX- plainable AI approaches for debugging and diagno- sis. Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using annotator rationales to improve machine learning for text categorization. In Human Language Technologies 2007 The Conference of the North American Chapter of the Association for Computa- tional Linguistics Proceedings of the Main Confer- ence, pages 260267, Rochester, New York. Associa- tion for Computational Linguistics. Matthew D. Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. In Com- puter Vision ECCV 2014, pages 818833, Cham. Springer International Publishing. Jianming Zhang, Zhe L. Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. 2016. Top-down neural at- tention by excitation backprop. International Journal of Computer Vision, 12610841102. Yu Zhang, Peter Tino, Ales Leonardis, and Ke Tang. 2021. A survey on neural network interpretability. IEEE Transactions on Emerging Topics in Computa- tional Intelligence, 5(5)726742. Bolei Zhou, Aditya Khosla, Àgata Lapedriza, Aude Oliva, and Antonio Torralba. 2014. Object detectors emerge in deep scene cnns. CoRR, abs1412.6856. A Experimental Details The implementations of MaRC for the exper- iments conducted in this study is available at httpsgithub.cominas-argumentation Explainability. A.1 Rationale Extraction We perform rationale detection using BERTbase (uncased) (Devlin et al., 2019), which we train as a binary classifier for at most 20 epochs on the first eight folds of the movie review dataset (Zaidan et al., 2007), with the ninth and tenth fold being used for validation and testing, respectively. For the optimization, we use the Adam optimizer and achieve a 96.5 test set accuracy. For rationale creation, the results from Table 1 as well as the example images for MaRC were created by using the optimization objective given by Equation 8 with all specifications as described in Section 3.2, hyperparameters set to αλ 1, ασ 1.2 and w and σ being uniformly initialized to 1.2 and 2, respectively. We add zero-mean Gaus- sian noise to x and xc (σ 0.03) and randomly set 5 of mask values to 0 or 1, respectively, in each optimization step. We use the log-likelihood of the respective class as scoring function. All these choices were made by using the validation split, with the",
    "metadata": {
      "document_id": 1,
      "chunk_id": 15,
      "source": "pdf_1",
      "chunk_length": 3459
    },
    "id": 34
  },
  {
    "text": "add zero-mean Gaus- sian noise to x and xc (σ 0.03) and randomly set 5 of mask values to 0 or 1, respectively, in each optimization step. We use the log-likelihood of the respective class as scoring function. All these choices were made by using the validation split, with the measure of quality being visual co- herence of the created explanation, as the data set does not offer a validation split with the same label distribution, thus making validation with respect to scores infeasible. Texts that surpass the limit of 510 input tokens for BERTbase are split into multiple segments, with consecutive segments overlapping for 100 tokens, and a separate mask is predicted for each segment. The resulting masks are con- catenated, with the overlapping parts being linearly blended. We proceed in the same way for all other interpretability methods. The following models and parameters were used in the method comparison Occlusion (Zeiler and Fergus, 2014) We chose to mask slightly larger spans of 5 to- kens as this produced smoother masks which resulted in higher IoU F1 scores. Occluded parts were replaced by PAD-tokens. Saliency (Simonyan et al., 2013) No special hyperparameter settings required. InXGrad (Input times gradient, Shrikumar et al., 2017) No special hyperparameter set- tings required. Int. Grads (Integrated Gradients, Sundarara- jan et al., 2017) We use a sequence of PAD- tokens as background and do 50 gradient eval- uation steps per sample. LIME (Ribeiro et al., 2016) We do 50 func- tion evaluations per sample. In each evalua- tion, we randomly select 5 13 of tokens and replace them as well as the next three tokens with PAD-tokens. We train a linear classifier and use the resulting weights as ra- tionale. Shapley (Shapley value sampling, Castro et al., 2009) We evaluate the token contributions for 25 feature permutations per sample. Removed tokens are replaced by PAD-tokens. We use the implementations provided by (Kokhlikyan et al., 2020) for all methods. All methods have access to the ground truth label and therefore do not have to rely on a correct classifier prediction. For methods that produce scores for each en- try of the embedding vector, we report results for two different methods of combining these scores to single values per token, with one being taking the vector norm (results are reported for the L1 norm, but we did not see a significant difference for the L2 norm), and the other one being summing over the resulting scores (indicated by subscript n and s in Table 1, respectively). In the latter case, the resulting value was negated if the target label is 0. To evaluate the token F1 score and the IoU F1 score, we need to create a binary mask from the continuous scores produced by the different inter- pretability methods. We do this by selecting the top-scoring words as rationale, with the percent- age of words that are selected being decided by a Nadaraya-Watson kernel regression model using an RBF kernel. The input to the kernel regression for a given sample is the percentage of words that have a",
    "metadata": {
      "document_id": 1,
      "chunk_id": 16,
      "source": "pdf_1",
      "chunk_length": 3064
    },
    "id": 35
  },
  {
    "text": "methods. We do this by selecting the top-scoring words as rationale, with the percent- age of words that are selected being decided by a Nadaraya-Watson kernel regression model using an RBF kernel. The input to the kernel regression for a given sample is the percentage of words that have a score greater than a fixed threshold (a hy- perparameter, here set to 0.1), while the output is the percentage of words to be selected as rationale. As we use the rationales from (DeYoung et al., 2020) (who only annotated 200 samples) for our experiment, we do not have access to a separate training set to train the kernel regression, so we resort to a leave-one-out scheme to use the same set for training and testing. For the faithfulness evaluation, we note that we deviate from the common practice of evaluating the area under the curve (AUC) (e.g., used by Pet- siuk et al., 2018) and instead take the average over the tested range of values. We do this, to accom- modate for the possibility of negative scores in the sufficiency calculation, which undermine the theoretical foundation of the AUC. We also adapt the comprehensiveness calculation accordingly for consistency. A.2 ImageNet Explanations We use MaRC with the optimization objective given by Equation 9. We use pretrained ResNet-101 (He et al., 2016) and vision transformer ViT-B16 (Kolesnikov et al., 2021, input image size384) models and use the following hyperparameter set- ting for MaRC ResNet-101 We set αλ 0.6, ασ 1.2, αNB 10 and initialize w and σ uniformly to 0.5 and 1.2 respectively. As ResNet mod- els seem to treat uniformly colored images as uninformative, we chose B to be a set con- taining a black image, a white image, and an image with the mean color from the dataset. As the scoring function, we chose the log of a combination of the softmax output of c (weighted by 0.9) and the sigmoid of the logit of c (weighted by 0.1). Vit-B16 We set αλ 0.25, ασ 1.2, αNB 10 and initialize w and σ uniformly to 0.5 and 1.2 respectively. As the vision trans- former often seems to interpret the uniformly colored backgrounds as indicative of specific classes, we instead opted to use a blurred ver- sion of the input image as b. As the scoring function, we use the log-likelihood of c. All visualizations and experiments were, if not stated otherwise, conducted with these hyperparam- eter settings. We compared MaRC to the following methods M-Perturb (Fong and Vedaldi, 2017) We used the original implementation and parameter set- tings by Fong and Vedaldi (2017) with minor adaptations to work with pytorch. Grad-CAM (Selvaraju et al., 2017) We used the implementation by Gildenblat and contrib- utors (2021). Exc-BP (Excitation Backpropaga- tion, Zhang et al., 2016) We used the implementation available at httpsgithub.comgreydanusexcitationbp Saliency (Simonyan et al., 2013) We used the implementation by Kokhlikyan et al. (2020). InputXGrad (Input times gradient, Shrikumar et al., 2017) We used the implementation by Kokhlikyan et al. (2020). Guided BP (Springenberg et al., 2015) We used the implementation by",
    "metadata": {
      "document_id": 1,
      "chunk_id": 17,
      "source": "pdf_1",
      "chunk_length": 3084
    },
    "id": 36
  },
  {
    "text": "2016) We used the implementation available at httpsgithub.comgreydanusexcitationbp Saliency (Simonyan et al., 2013) We used the implementation by Kokhlikyan et al. (2020). InputXGrad (Input times gradient, Shrikumar et al., 2017) We used the implementation by Kokhlikyan et al. (2020). Guided BP (Springenberg et al., 2015) We used the implementation by Kokhlikyan et al. (2020). Intgr. Grads (Integrated Gradients, Sundarara- jan et al., 2017) We used the implementation by Kokhlikyan et al. (2020). As background, a blurred version of the input image was used, as this produced optimal results. For each sample, 100 gradient evaluation steps were performed. Occlusion (Zeiler and Fergus, 2014) We used the implementation by Kokhlikyan et al. (2020). We occluded patches of 1 9 of the input image size and used a stride of 1 56 of the input image size. Occluded patches were replaced by a blurred version of the input image. Attention (Yuan et al., 2021) We used the implementation by Yuan et al. (2021). Attribution (Chefer et al., 2021b) We used the implementation by Yuan et al. (2021). Rollout (Abnar and Zuidema, 2020) We used the implementation by Yuan et al. (2021). TAM (Transition attention maps, Yuan et al., 2021) We used the implementation by Yuan et al. (2021). All methods had access to the ground-truth label to create the explanation. The random sample of ImageNet validation images that was used in this experiment can be viewed at the GitHub page given at the beginning of Appendix A. To evaluate model faithfulness, we used Equa- tion 10 and 11, with the difference that the evalua- tion for each sample and percentage was performed with respect to four uninformative inputs to prevent skewed evaluation results for methods that only work well with respect to one background. The four backgrounds we used were a white image, a black image, an image of the mean color of the dataset, as well as a blurred version of the input image. B Movie Review Rationale Examples This section includes four additional exemplary ra- tionales created by MaRC on movie reviews from (Zaidan et al., 2007). Figure 4 Rationale for a negative movie review. Figure 5 Rationale for a positive movie review. Figure 6 Rationale for a negative movie review. Figure 7 Rationale for a positive movie review. C ImageNet Mask Comparison This section includes a more extensive comparison of masks created by different interpretability meth- ods on the selection of images used by Fong and Vedaldi (2017). For MaRC, an additional visual- ization is added to more easily see the unmasked regions in the image. For an overview of meth- ods as well as hyperparameter settings, compare Appendix A. C.1 ResNet-101 Input MaRC M-Perturb Grad-CAM Excitation BP Occlusion MaRC Vis. Saliency InputXGrad Guided BP Integr. Grads Figure 8 Komodo dragon Input MaRC M-Perturb Grad-CAM Excitation BP Occlusion MaRC Vis. Saliency InputXGrad Guided BP Integr. Grads Figure 9 Pekinese Input MaRC M-Perturb Grad-CAM Excitation BP Occlusion MaRC Vis. Saliency InputXGrad Guided BP Integr. Grads Figure 10 Impala Input MaRC M-Perturb Grad-CAM Excitation BP Occlusion MaRC Vis. Saliency InputXGrad Guided BP Integr. Grads",
    "metadata": {
      "document_id": 1,
      "chunk_id": 18,
      "source": "pdf_1",
      "chunk_length": 3186
    },
    "id": 37
  },
  {
    "text": "MaRC M-Perturb Grad-CAM Excitation BP Occlusion MaRC Vis. Saliency InputXGrad Guided BP Integr. Grads Figure 9 Pekinese Input MaRC M-Perturb Grad-CAM Excitation BP Occlusion MaRC Vis. Saliency InputXGrad Guided BP Integr. Grads Figure 10 Impala Input MaRC M-Perturb Grad-CAM Excitation BP Occlusion MaRC Vis. Saliency InputXGrad Guided BP Integr. Grads Figure 11 Squirrel monkey Input MaRC M-Perturb Grad-CAM Excitation BP Occlusion MaRC Vis. Saliency InputXGrad Guided BP Integr. Grads Figure 12 CD player Input MaRC M-Perturb Grad-CAM Excitation BP Occlusion MaRC Vis. Saliency InputXGrad Guided BP Integr. Grads Figure 13 Pickup Input MaRC M-Perturb Grad-CAM Excitation BP Occlusion MaRC Vis. Saliency InputXGrad Guided BP Integr. Grads Figure 14 Sunglasses Input MaRC M-Perturb Grad-CAM Excitation BP Occlusion MaRC Vis. Saliency InputXGrad Guided BP Integr. Grads Figure 15 Unicycle Input MaRC M-Perturb Grad-CAM Excitation BP Occlusion MaRC Vis. Saliency InputXGrad Guided BP Integr. Grads Figure 16 Street sign Input MaRC M-Perturb Grad-CAM Excitation BP Occlusion MaRC Vis. Saliency InputXGrad Guided BP Integr. Grads Figure 17 Chocolate sauce Input MaRC M-Perturb Grad-CAM Excitation BP Occlusion MaRC Vis. Saliency InputXGrad Guided BP Integr. Grads Figure 18 Cliff C.2 ViT-B16 Input MaRC M-Perturb Raw attention Rollout Attribution TAM MaRC Vis. Saliency InputXGrad Integr. Grads Occlusion Grad-CAM Figure 19 Komodo dragon Input MaRC M-Perturb Raw attention Rollout Attribution TAM MaRC Vis. Saliency InputXGrad Integr. Grads Occlusion Grad-CAM Figure 20 Pekinese Input MaRC M-Perturb Raw attention Rollout Attribution TAM MaRC Vis. Saliency InputXGrad Integr. Grads Occlusion Grad-CAM Figure 21 Impala Input MaRC M-Perturb Raw attention Rollout Attribution TAM MaRC Vis. Saliency InputXGrad Integr. Grads Occlusion Grad-CAM Figure 22 Squirrel monkey Input MaRC M-Perturb Raw attention Rollout Attribution TAM MaRC Vis. Saliency InputXGrad Integr. Grads Occlusion Grad-CAM Figure 23 CD player Input MaRC M-Perturb Raw attention Rollout Attribution TAM MaRC Vis. Saliency InputXGrad Integr. Grads Occlusion Grad-CAM Figure 24 Pickup Input MaRC M-Perturb Raw attention Rollout Attribution TAM MaRC Vis. Saliency InputXGrad Integr. Grads Occlusion Grad-CAM Figure 25 Sunglasses Input MaRC M-Perturb Raw attention Rollout Attribution TAM MaRC Vis. Saliency InputXGrad Integr. Grads Occlusion Grad-CAM Figure 26 Unicycle Input MaRC M-Perturb Raw attention Rollout Attribution TAM MaRC Vis. Saliency InputXGrad Integr. Grads Occlusion Grad-CAM Figure 27 Unicycle Input MaRC M-Perturb Raw attention Rollout Attribution TAM MaRC Vis. Saliency InputXGrad Integr. Grads Occlusion Grad-CAM Figure 28 Chocolate sauce Input MaRC M-Perturb Raw attention Rollout Attribution TAM MaRC Vis. Saliency InputXGrad Integr. Grads Occlusion Grad-CAM Figure 29 Cliff",
    "metadata": {
      "document_id": 1,
      "chunk_id": 19,
      "source": "pdf_1",
      "chunk_length": 2853
    },
    "id": 38
  },
  {
    "text": "Dataset Creation for Visual Entailment using Generative AI Rob Reijtenbach Leiden University rob.reijtenbachgmail.com Suzan Verberne Leiden University (s.verberne Gijs Wijnholds Leiden University g.wijnholds)liacs.leidenuniv.nl Abstract In this paper we present and validate a new synthetic dataset for training visual entailment models. Existing datasets for visual entailment are small and sparse compared to datasets for textual entailment. Manually creating datasets is labor-intensive. We base our synthetic dataset on the SNLI dataset for textual entail- ment. We take the premise text from SNLI as input prompts in a generative image model, Stable Diffusion, creating an image to replace each textual premise. We evaluate our dataset both intrinsically and extrinsically. For extrin- sic evaluation, we evaluate the validity of the generated images by using them as training data for a visual entailment classifier based on CLIP feature vectors. We find that synthetic train- ing data only leads to a slight drop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when trained on real data. We also compare the quality of our generated training data to original training data on another dataset SICK-VTE. Again, there is only a slight drop in F-score from 0.400 to 0.384. These re- sults indicate that in settings with data sparsity, synthetic data can be a promising solution for training visual entailment models. 1 Introduction Natural language inference (NLI) is a classification problem for pairs of two texts, a premise and a hypothesis. The pair is labeled as entailment (the premise entails the hypothesis), neutral or contra- diction (the hypothesis contradicts the premise). In visual entailment (VE) tasks (Xie et al., 2019), the premise is substituted by an image, while the hypothesis is still in text form. In order to create and train effective models for VE, large datasets are needed. While datasets of images combined with hypotheses and labels do exist, they are relatively small and sparse com- pared to datasets for textual entailment. Existing datasets are SNLI-VE (Xie et al., 2019) and SICK- VTE (Iokawa et al., 2024) which are both based on NLI datasets and which were created by manual la- bor leveraging Amazon Mechanical Turk workers. In this paper we evaluate the use of generative AI for VE dataset creation which would allow cheaper and easier dataset creation. This is done by first generating a synthetic dataset, of which we then verify the validity. We introduce a synthetic version of the SNLI-VE dataset called Synthetic-NLI-VE and show how models trained on this dataset have similar performance when tested on real data com- pared to models trained on real data. In summary, the contributions of this paper are threefold (1) we present the new dataset Synthetic- NLI-VE1 (2) we find that the performance of mod- els trained on the generated dataset have similar performance compared to models trained on real data (3) A cross-data evaluation shows that gener- alizability of visual entailment models to a different dataset is poor, whether or not the training set was generated or original. 2 Related work Visual entailment and dataset creation The idea of visual entailment was first proposed by",
    "metadata": {
      "document_id": 2,
      "chunk_id": 0,
      "source": "pdf_2",
      "chunk_length": 3254
    },
    "id": 39
  },
  {
    "text": "models trained on real data (3) A cross-data evaluation shows that gener- alizability of visual entailment models to a different dataset is poor, whether or not the training set was generated or original. 2 Related work Visual entailment and dataset creation The idea of visual entailment was first proposed by Xie et al. (2019). For this task they introduce the Ex- plainable Visual Entailment (EVE) model, based on Attention Visualization. In the same paper the authors introduce the SNLI-VE dataset (Section 3). Antol et al. (2015) introduced a dataset for visual question answering (QA). They used the Microsoft Common Objects in Context (MS COCO) dataset (Lin et al., 2014) as a starting point 200k im- ages of real-world scenes with 5 captions per im- age. They added 50k images of abstract scenes for which they also collected 5 captions per image. Marelli et al. (2014) created the SICK dataset. SICK (sentences involving compositional knowl- 1httpshuggingface.codatasets robreijtenbachSynthetic-NLI-VE arXiv2508.11605v1 cs.CL 15 Aug 2025 edge) contains sentence pairs with both relatedness scores and entailment labels. This dataset was cre- ated by pairing the Flickr8K dataset (Hodosh et al., 2013) and the SemEval-2012 STS data (Agirre et al., 2012) and having Amazon Mechanical Turk workers annotate them with both similarity scores and entailment labels. Wijnholds and Moortgat (2021) created the Dutch version of SICK using a semi-automatic translation. Bowman et al. (2015) introduced the SNLI dataset on which the afore- mentioned SNLI-VE was based, with as motivation that the SICK dataset is too small and not balanced enough. For SNLI they created a balanced dataset of around 500k sentence pairs compared to the 10k in the SICK dataset. There are also efforts made to improve existing datasets. This was already the case with Goyal et al. (2017), who improved and extended the VQA dataset resulting in the VQA-v2 dataset. The dataset was improved by, among other things, re- ducing bias and extended it by adding more images. This has also been done for the SNLI-VE dataset by Do et al. (2021) who created the e-SNLI-VE-2.0. Synthetic data Unlike the largely human made datasets that were previously discussed, the CLEVR dataset (Johnson et al., 2016) is automat- ically generated. This dataset contains images of abstract shapes combined with automatically gen- erated questions. The images were created by ran- domly sampling a scene graph and rendering it us- ing the open-source 3D rendering software Blender. Yuan et al. (2024) proposed an evaluation frame- work for assessing synthetic data generated by large language models (LLMs). This framework in- cludes measures for fidelity, utility and privacy. In this work, we only focus on the fidelity and utility of the generated data. Some research suggests that using synthetic datasets for model training could have a negative effect on performance in the future, if generated datasets are used for training computer vision mod- els (Hataya et al., 2023). As opposed to synthetic datasets used to train generative models, the im- ages that we generate are used to train classification models. Furthermore, these classification models are evaluated on original data, ensuring good real",
    "metadata": {
      "document_id": 2,
      "chunk_id": 1,
      "source": "pdf_2",
      "chunk_length": 3261
    },
    "id": 40
  },
  {
    "text": "future, if generated datasets are used for training computer vision mod- els (Hataya et al., 2023). As opposed to synthetic datasets used to train generative models, the im- ages that we generate are used to train classification models. Furthermore, these classification models are evaluated on original data, ensuring good real world generalizability. 3 Data In this work we use two datasets which we briefly describe in this section. SNLI-VE This was introduced by (Xie et al., 2019), by combining the SNLI dataset (Bowman et al., 2015) with the Flickr30k dataset (Young et al., 2014). The Flickr30k dataset was created by taking 31,783 photos of everyday activities which were harvested from Flickr. Each image receives 5 different captions resulting in 158,915 captions in total. Figure 3 in the appendix shows an example of an image and its captions. The SNLI dataset (Bowman et al., 2015) is a well known dataset specifically created for natural language inference. In short, it was constructed by having Amazon Mechanical Turk workers gener- ate 3 hypotheses per caption, where captions came from the Flickr30k dataset. From this, Xie et al. (2019) could therefore create the SNLI-VE dataset by replacing each premise by the original corre- sponding image. The dataset contains a total of 31,783 images, 157,567 premises and 565,286 hy- potheses. SICK-VTE Along the lines of the creation of SNLI-VE, Iokawa et al. (2024) introduces SICK- VTE, a visual entailment version of (a subset of) the SICK dataset (Marelli et al., 2014), but with an additional multilingual component, including also the Dutch (Wijnholds and Moortgat, 2021) and Japanese (Yanaka and Mineshima, 2022) trans- lations of the SICK dataset. The construction of the original SICK dataset was based on sentence transformation rules over image captions instead of human-generated hypothesis. By construction the dataset contains only cases of Entailment and Con- tradiction for 488 unique images there are 2,899 sentence pairs, with 1,930 examples of Entailment and 969 examples of Contradiction. 4 Methods We generate a synthetic dataset as described in 4.1. We then report on the intrinsic evaluation of image quality by comparing the generated images directly with the original images based on a similarity analy- sis in 4.2. Finally, we perform extrinsic evaluation of synthetic data, comparing it to original data for visual entailment model training in 4.3. 4.1 Image Generation Our approach for creating the generated dataset is to use the premise text from SNLI as input prompts in a generative model, creating an image for every premise caption. This results in a dataset similar to SNLI-VE, however, instead of multiple premises referencing the same image, here the resulting dataset has a unique image for every premise. We refer to the generated images as child images to ex- press the fact that they were indirectly derived from an original parent image. Examples of generated child images are shown in Figure 1. Our choice of generative model is Stability AIs Stable Diffusion2. The ability to run the model locally as opposed to the cloud based solutions from OpenAI and Midjourney was essential for generating the large amount of",
    "metadata": {
      "document_id": 2,
      "chunk_id": 2,
      "source": "pdf_2",
      "chunk_length": 3218
    },
    "id": 41
  },
  {
    "text": "an original parent image. Examples of generated child images are shown in Figure 1. Our choice of generative model is Stability AIs Stable Diffusion2. The ability to run the model locally as opposed to the cloud based solutions from OpenAI and Midjourney was essential for generating the large amount of images necessary for our work. The chosen resolution was square images of 512x512 pixels as this is the image size Stable Diffusion was trained on and it is close to the aver- age image size of the original SNLI-VE dataset.3 The checkpoint chosen for this research is Realistic Vision v514 which was finetuned for generating photorealistic images. 4.2 Intrinsic evaluation To assess intrinsic image quality we rely on two measures. As an initial verification we compute pairwise cosine similarity between the CLIP fea- ture vectors of original and generated images and assess the distribution of these values, expecting to see a normal distribution. Secondly, we use ranked similarity scores over the full dataset to inspect whether, for a given origi- nal image, the 5 generated images for it will appear as highly similar or not. We specifically use re- callk and precisionk for evaluation In the ranking problem in this work, we take the query to be an original image, and the ranked list of documents to be the 100 most similar generated images as determined by cosine similarity. The relevance function is now binary, returning 1 for an image that was indeed generated from one of the captions of the original image, and 0 otherwise. For precisionk, we divide the true positives by the number of retrieved images. 4.3 Extrinsic evaluation We test the validity of the generated images by us- ing them as training data for a classifier to learn the visual entailment classification problem. The 2httpsgithub.comStability-AI generative-models 3The mean width and height were 459 and 395 respectively, and the standard deviations were 67 for width and 74 for height with both having a maximal value of exactly 500. 4httpshuggingface.costablediffusionapi realistic-vision-v51 approach for this experiment is based on Song et al. (2022) who proposed using CLIP for visual en- tailment. Their method includes taking the CLIP feature vector of both the premise image and the hy- pothesis text, fusing these according to Equation 1 and training an MLP on this fused vector represen- tation to output the correct entailment label. fuse(v1, v2) v1, v2, v1v2, v1v2, v1v2 (1) The input dimension for this perceptron is 2560 which is a direct result of the output size of the fuse function. The fuse function concatenates the feature vector of the image, the feature vector of the hypothesis, the sum of these two vectors as well as the difference between these vectors and finally the product of these vectors. This results in a total of five vectors that are concatenated and with each vector having a size of 512 numbers, the result has a length of 5 512 2560. The resulting vector is used as an input for the MLP which has one hidden layer of size 250. After experimenting with different layer",
    "metadata": {
      "document_id": 2,
      "chunk_id": 3,
      "source": "pdf_2",
      "chunk_length": 3092
    },
    "id": 42
  },
  {
    "text": "total of five vectors that are concatenated and with each vector having a size of 512 numbers, the result has a length of 5 512 2560. The resulting vector is used as an input for the MLP which has one hidden layer of size 250. After experimenting with different layer sizes, the size of this hidden layer did not seem to affect the accuracy of the classifier but had an impact on the compu- tational performance. After this one hidden layer the network only has one more layer which is the output layer. This output layer has a size of 3 cor- responding to the three possible labels entailment, neutral, contradiction. We use this method to train classifiers on both the the original images and the generated images of the SNLI-VE dataset. These classifiers are then tested on the original as well as on the generated test sets, after which their performance is compared. Note that absolute performance of the classifier is not the primary goal. Rather, we are interested in the relative performance of a classifier trained on generated images compared to a classifier trained on real images. We, however, aim for good perfor- mance of both as this yields the most accurate data to compare between these two. 5 Experiments and Results In this section we first report on the results for the intrinsic evaluation (5.1), after which we discuss the downstream performance in the Visual Entail- ment task (5.2), and finally we discuss the results of transferring the Visual Entailment model to the SICK-VTE dataset (5.3). A wedding party walks out of a building. The group of people are assembling for a wedding. A man and woman dressed for a wedding function. Figure 1 Three examples of generated images based on three of the captions in Figure 3. 5.1 Intrinsic evaluation The starting point of our intrinsic comparison is the cosine similarity distribution for images in the development and test set of the SNLI-VE dataset and its generated child images. Each original im- age is compared to all the generated images and the similarity scores are saved. We found that the simi- larity values follow a normal distribution for both the development and test set. The mean for both sets is 0.465 with a standard deviation of 0.085 This is also illustrated in Figure 4 in the appendix. Ranked similarity After assessing the similarity distribution between original and generated images, we report on the recallk and precisionk curves. Initially, we computed average recallk and preci- sionk values for k 100, which reveals that on average only 1.6 of the 100 most similar synthetic images to the real images were based actually gen- erated based on one of the premises accompanying that real image. These results stem from the fact that finding the 100 most similar out of 160k gen- erated images will likely not result in finding all of the 5 images that are relevant. This is illustrated in Figure 2 where an image is shown together with the most cosine similar generated image which is not one of",
    "metadata": {
      "document_id": 2,
      "chunk_id": 4,
      "source": "pdf_2",
      "chunk_length": 2991
    },
    "id": 43
  },
  {
    "text": "that finding the 100 most similar out of 160k gen- erated images will likely not result in finding all of the 5 images that are relevant. This is illustrated in Figure 2 where an image is shown together with the most cosine similar generated image which is not one of its child images. These two images could be considered rather similar by a human. It is likely that there are more images in the collec- tion that are similar than only the child images, making the recallk measure an underestimation of the real quality of the generated images. The recallk and precisionk curves for this setting are in Figures 5a and 5b in the appendix. To get a fairer picture of the similarity evaluation, we recalculate recallk and precisionk curves for a sampled version of the data which is needed as the train set is large very large compared to the dev and test set, which are only 1000 original images Train set Original Generated Original 70.3 0.703 71.1 0.710 Generated 68.9 0.686 73.2 0.732 Table 1 AccuraciesF1 scores of both models on both test sets of SNLI-VE. each. We randomly sample 1000 examples from the train set of SNLI-VE, and consequently cal- culate recallk and precisionk values for train, development, and test sets separately, each time considering 1000 original images and its 5000 generated child images. The resulting plots for the recallk and precisionk of the samples are in Figure 6b and Figure 6a in the appendix. We find that the average success rate is between 3.5 and 4 out of the five possible relevant images, indicat- ing that most of the relevant real images are found within the first 100 most similar generated images. For completeness, we include the variance of the recall and precision curves of the samples in Fig- ure 7 in the appendix where one standard deviation above and below each curve is marked. 5.2 Extrinsic Evaluation Classification We train both a model on the dataset of original images, and a model on the dataset of generated images, using the same traindevtest split as sug- gested for the SNLI-VE dataset. We trained the model for 100 epochs and selecting the epoch for which the model performs highest on the develop- ment set, which was saved for evaluating on the test set. The accuracy and loss on the training set and dev set are shown in the appendix in Figure 8a and 8b and Figure 9a and 9b respectively. We report accuracies and F1 scores in Table 1. We observe the best overall performance when us- ing the model trained on generated data evaluated on the generated data as well. This suggests that (a) Original (b) Generated Figure 2 An example of an image and a generated image which looks similar but is not considered relevant as the generated image is not a child of the original image in this evaluation. The original image (a) had 5 captions in the dataset written by 5 different workers. Image (b) was generated for the caption A group of young men",
    "metadata": {
      "document_id": 2,
      "chunk_id": 5,
      "source": "pdf_2",
      "chunk_length": 2914
    },
    "id": 44
  },
  {
    "text": "which looks similar but is not considered relevant as the generated image is not a child of the original image in this evaluation. The original image (a) had 5 captions in the dataset written by 5 different workers. Image (b) was generated for the caption A group of young men have finished their drinks while sitting at a table in a restaurant . the generated images and their classification has less variability compared to the original data. We also see that the model trained on original images performs better on the generated test set than it does on the original test set. This could suggest that the generated test set is easier to classify. Lastly, and most importantly, we do see that the model trained on generated data and tested on orig- inal data has a somewhat lower performance in this experiment, but the difference is small. It suggests that synthetic training data results in slightly worse performance in real world tasks. 5.3 Cross-data generalizability The final part of the experiments evaluate the per- formance of the trained models when they are tested on another dataset, in this case the SICK- VTE dataset. As discussed in Section 3, SICK- VTE and its synthetic counterpart do not contain any neutral examples. To train visual entailment models, having neutral examples would be essen- tial however for the purpose of testing the general- izability pretrained models, a dataset with neutral examples is preferred. The experimental setup is similar to that of the classification experiment in Section 5.2, except that we now reuse the trained models from the prior experiment as we assess transfer capabilities. Both of the trained models were tested on the original SICK-VTE dataset and, for completeness, also on the generated version of SICK-VTE. Similar to the previous experiment, we report both accuracy and F1 scores in Table 2. Note that, in contrast to the results on the SNLI-VE dataset, accuracy and F1 scores diverge, due to label imbalance in SICK-VTE. Train set Original Generated Original 50.7 0.400 51.4 0.391 Generated 47.2 0.384 47.6 0.384 Table 2 AccuraciesF1 scores of both models on the SICK-VTE datasets. We find that performance is relatively poor, given a majority baseline of 0.6657 for a model only predicting Entailment. This result is in line with the findings of Talman and Chatzikyriakidis (2019), who found similar issues when transfering models trained on the SNLI dataset to the SICK dataset. Secondly, we can conclude that the model trained on generated data performs slightly worse compared to the model trained on original data. This is in line with the findings in the previous experiment (5.2). 6 Conclusion In this paper we introduced a synthetic VE dataset Synthetic-NLI-VE. The dataset proved to have sim- ilar utility compared to the dataset it was based on while being far less costly to create. This also proves the viability of using generative AI to create datasets for the VE task, whereby we pave the way for future research into using synthetic data for VE dataset creation. As future work we propose chang- ing the",
    "metadata": {
      "document_id": 2,
      "chunk_id": 6,
      "source": "pdf_2",
      "chunk_length": 3091
    },
    "id": 45
  },
  {
    "text": "was based on while being far less costly to create. This also proves the viability of using generative AI to create datasets for the VE task, whereby we pave the way for future research into using synthetic data for VE dataset creation. As future work we propose chang- ing the single set of parameters for the generation model to a variety of different values. Secondly, generating more than one image per caption could result in better training data compared to the one image per caption dataset we generated. Lastly, evaluating different classification algorithms could further strengthen the findings. Limitations Our experiments are limited evaluation for the CLIP model, and the findings might be different for other visual entailment models. We investigated cross-data generalizability in synthetic VTE datasets. One limitation of our ex- periments is that both SNLI-VE and SICK-VTE are created based on Flickr30K, which makes them relatively more similar to each other than datasets based on other sources, such as NLVR and NLVR2.5 We leave this cross-domain evaluation for future work. References Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 task 6 A pilot on semantic textual similarity. In SEM 2012 The First Joint Conference on Lexical and Compu- tational Semantics Volume 1 Proceedings of the main conference and the shared task, and Volume 2 Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385 393, Montréal, Canada. Association for Computa- tional Linguistics. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar- garet Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA visual question an- swering. CoRR, abs1505.00468. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empiri- cal Methods in Natural Language Processing, pages 632642, Lisbon, Portugal. Association for Compu- tational Linguistics. Virginie Do, Oana-Maria Camburu, Zeynep Akata, and Thomas Lukasiewicz. 2021. e-snli-ve-2.0 Corrected visual-textual entailment with natural language ex- planations. CoRR, abs2004.03744. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in vqa matter Elevating the role of image un- derstanding in visual question answering. CoRR, abs1612.0083763256334. Ryuichiro Hataya, Han Bao, and Hiromi Arai. 2023. Will large-scale generative models corrupt future datasets? In IEEECVF International Conference on Computer Vision, ICCV 2023, Paris, France, Oc- tober 1-6, 2023, pages 2049820508. IEEE. Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing image description as a ranking task data, models and evaluation metrics. Journal of Arti- ficial Intelligence Research, 47(1)853899. 5httpslil.nlp.cornell.edunlvr Nobuyuki Iokawa, Gijs Wijnholds, and Hitomi Yanaka. 2024. Multilingual visual-textual entail- ment benchmark with diverse linguistic phenom- ena. Proceedings of the Annual Conference of JSAI, JSAI20244C3GS11044C3GS1104. Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. 2016. CLEVR A diagnostic dataset for compositional language and elementary visual rea- soning. CoRR, abs1612.06890. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft COCO com- mon objects",
    "metadata": {
      "document_id": 2,
      "chunk_id": 7,
      "source": "pdf_2",
      "chunk_length": 3576
    },
    "id": 46
  },
  {
    "text": "and Ross B. Girshick. 2016. CLEVR A diagnostic dataset for compositional language and elementary visual rea- soning. CoRR, abs1612.06890. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft COCO com- mon objects in context. CoRR, abs1405.0312. Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zam- parelli. 2014. A SICK cure for the evaluation of compositional distributional semantic models. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC14), pages 216223, Reykjavik, Iceland. European Lan- guage Resources Association (ELRA). Haoyu Song, Li Dong, Weinan Zhang, Ting Liu, and Furu Wei. 2022. CLIP models are few-shot learn- ers Empirical studies on VQA and visual entailment. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1 Long Papers), pages 60886100, Dublin, Ireland. Association for Computational Linguistics. Aarne Talman and Stergios Chatzikyriakidis. 2019. Testing the generalization power of neural network models across NLI benchmarks. In Proceedings of the 2019 ACL Workshop BlackboxNLP Analyzing and Interpreting Neural Networks for NLP, pages 85 94, Florence, Italy. Association for Computational Linguistics. Gijs Wijnholds and Michael Moortgat. 2021. SICK-NL A dataset for Dutch natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Lin- guistics Main Volume, pages 14741479, Online. Association for Computational Linguistics. Ning Xie, Farley Lai, Derek Doran, and Asim Ka- dav. 2019. Visual entailment A novel task for fine-grained image understanding. CoRR, abs1901.06706. Hitomi Yanaka and Koji Mineshima. 2022. Compo- sitional evaluation on Japanese textual entailment and similarity. Transactions of the Association for Computational Linguistics, 1012661284. Peter Young, Alice Lai, Micah Hodosh, and Julia Hock- enmaier. 2014. From image descriptions to visual denotations New similarity metrics for semantic in- ference over event descriptions. Transactions of the Association for Computational Linguistics, 26778. Yefeng Yuan, Yuhong Liu, and Liang Cheng. 2024. A multi-faceted evaluation framework for assessing synthetic data generated by large language models. Preprint, arXiv2404.14445. Appendix Additional figures are on the following pages. A bearded man, and a girl in a red dress are getting married. A wedding party walks out of a building. The group of people are assembling for a wedding. A man and woman dressed for a wedding function. A woman holds a mans arm at a formal event. Figure 3 One of the 30k photos and its 5 accompanying captions from the SNLI dataset. Figure 4 Cosine similarity values for the dataset, showing the expected normal distribution. (a) (b) Figure 5 Recall (a) and precision (b) curves, calculated as averaged over the full dataset of images. (a) (b) Figure 6 Precision and recall curves where the train set is sampled in samples of 1000 images. (a) (b) Figure 7 Average precision and recall curves with one standard deviation. (a) (b) Figure 8 Performance on the training set during training. (a) (b) Figure 9 Performance on the development set after each epoch.",
    "metadata": {
      "document_id": 2,
      "chunk_id": 8,
      "source": "pdf_2",
      "chunk_length": 3376
    },
    "id": 47
  },
  {
    "text": "(b) Figure 7 Average precision and recall curves with one standard deviation. (a) (b) Figure 8 Performance on the training set during training. (a) (b) Figure 9 Performance on the development set after each epoch.",
    "metadata": {
      "document_id": 2,
      "chunk_id": 9,
      "source": "pdf_2",
      "chunk_length": 213
    },
    "id": 48
  }
]